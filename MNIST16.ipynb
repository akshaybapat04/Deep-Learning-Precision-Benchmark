{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fp32_storage_2/conv1   [128, 56, 56, 64]\n",
      "fp32_storage_2/pool1   [128, 27, 27, 64]\n",
      "fp32_storage_2/conv2   [128, 27, 27, 192]\n",
      "fp32_storage_2/pool2   [128, 13, 13, 192]\n",
      "fp32_storage_2/conv3   [128, 13, 13, 384]\n",
      "fp32_storage_2/conv4   [128, 13, 13, 256]\n",
      "fp32_storage_2/conv5   [128, 13, 13, 256]\n",
      "fp32_storage_2/pool5   [128, 6, 6, 256]\n",
      "2017-11-16 13:26:09.850388: step 0, duration = 0.075\n",
      "2017-11-16 13:26:17.404479: step 100, duration = 0.076\n",
      "2017-11-16 13:26:24.955540: step 200, duration = 0.076\n",
      "2017-11-16 13:26:32.512634: step 300, duration = 0.075\n",
      "2017-11-16 13:26:40.062730: step 400, duration = 0.076\n",
      "2017-11-16 13:26:47.605780: step 500, duration = 0.075\n",
      "2017-11-16 13:26:55.137815: step 600, duration = 0.075\n",
      "2017-11-16 13:27:02.685917: step 700, duration = 0.075\n",
      "2017-11-16 13:27:10.220929: step 800, duration = 0.075\n",
      "2017-11-16 13:27:17.770017: step 900, duration = 0.076\n",
      "2017-11-16 13:27:25.248902: Forward across 1000 steps, 0.075 +/- 0.000 sec / batch\n",
      "2017-11-16 13:27:48.831641: step 0, duration = 0.234\n",
      "2017-11-16 13:28:11.999245: step 100, duration = 0.234\n",
      "2017-11-16 13:28:35.183912: step 200, duration = 0.234\n",
      "2017-11-16 13:28:58.309420: step 300, duration = 0.234\n",
      "2017-11-16 13:29:21.518613: step 400, duration = 0.236\n",
      "2017-11-16 13:29:44.857265: step 500, duration = 0.235\n",
      "2017-11-16 13:30:08.153254: step 600, duration = 0.235\n",
      "2017-11-16 13:30:31.451194: step 700, duration = 0.236\n",
      "2017-11-16 13:30:54.748164: step 800, duration = 0.238\n",
      "2017-11-16 13:31:18.122356: step 900, duration = 0.233\n",
      "2017-11-16 13:31:41.253852: Forward-backward across 1000 steps, 0.233 +/- 0.002 sec / batch\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\keras\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2870: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ==============================================================================\n",
    "\n",
    "\"\"\"Timing benchmark for AlexNet inference.\n",
    "\n",
    "To run, use:\n",
    "  bazel run -c opt --config=cuda \\\n",
    "      models/tutorials/image/alexnet:alexnet_benchmark\n",
    "\n",
    "Across 100 steps on batch size = 128.\n",
    "\n",
    "Forward pass:\n",
    "Run on Tesla K40c: 145 +/- 1.5 ms / batch\n",
    "Run on Titan X:     70 +/- 0.1 ms / batch\n",
    "\n",
    "Forward-backward pass:\n",
    "Run on Tesla K40c: 480 +/- 48 ms / batch\n",
    "Run on Titan X:    244 +/- 30 ms / batch\n",
    "\"\"\"\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "from datetime import datetime\n",
    "import math\n",
    "import sys\n",
    "import time\n",
    "\n",
    "from six.moves import xrange  # pylint: disable=redefined-builtin\n",
    "import tensorflow as tf\n",
    "slim = tf.contrib.slim\n",
    "from tensorflow.contrib.slim.nets import resnet_v1\n",
    "from tensorflow.contrib.slim.nets import resnet_utils\n",
    "\n",
    "FLAGS = None\n",
    "\n",
    "\n",
    "\n",
    "def float32_variable_storage_getter(getter, name, shape=None, dtype=None, initializer=None, regularizer=None, trainable=True, *args, **kwargs): \n",
    "    \"\"\"Custom variable getter that forces trainable variables to be stored in float32 precision and then casts them to the training precision. \"\"\"\n",
    "    storage_dtype = tf.float32 if trainable else dtype \n",
    "    variable = getter(name, shape, dtype=storage_dtype, initializer=initializer, regularizer=regularizer, trainable=trainable, *args, **kwargs) \n",
    "    if trainable and dtype != tf.float32: \n",
    "        variable = tf.cast(variable, dtype) \n",
    "    return variable\n",
    "\n",
    "def gradients_with_loss_scaling(loss, variables, loss_scale): \n",
    "    \"\"\"Gradient calculation with loss scaling to improve numerical stability when training with float16. \"\"\" \n",
    "    return [grad / loss_scale for grad in tf.gradients(loss * loss_scale, variables)]\n",
    "\n",
    "\n",
    "def print_activations(t):\n",
    "  print(t.op.name, ' ', t.get_shape().as_list())\n",
    "\n",
    "\n",
    "def inference(images, dtype):\n",
    "  \"\"\"Build the AlexNet model.\n",
    "\n",
    "  Args:\n",
    "    images: Images Tensor\n",
    "\n",
    "  Returns:\n",
    "    pool5: the last Tensor in the convolutional component of AlexNet.\n",
    "    parameters: a list of Tensors corresponding to the weights and biases of the\n",
    "        AlexNet model.\n",
    "  \"\"\"\n",
    "  parameters = []\n",
    "  # conv1\n",
    "  with tf.name_scope('conv1') as scope:\n",
    "    kernel = tf.Variable(tf.truncated_normal([11, 11, 3, 64], dtype=dtype,\n",
    "                                             stddev=1e-1), name='weights')\n",
    "    conv = tf.nn.conv2d(images, kernel, [1, 4, 4, 1], padding='SAME')\n",
    "    biases = tf.Variable(tf.constant(0.0, shape=[64], dtype=dtype),\n",
    "                         trainable=True, name='biases')\n",
    "    bias = tf.nn.bias_add(conv, biases)\n",
    "    conv1 = tf.nn.relu(bias, name=scope)\n",
    "    print_activations(conv1)\n",
    "    parameters += [kernel, biases]\n",
    "\n",
    "  ## lrn1\n",
    "  #with tf.name_scope('lrn1') as scope:\n",
    "    #lrn1 = tf.nn.local_response_normalization(conv1,\n",
    "                                              #alpha=1e-4,\n",
    "                                              #beta=0.75,\n",
    "                                              #depth_radius=2,\n",
    "                                              #bias=2.0)\n",
    "\n",
    "  # pool1\n",
    "  pool1 = tf.nn.max_pool(conv1,\n",
    "                         ksize=[1, 3, 3, 1],\n",
    "                         strides=[1, 2, 2, 1],\n",
    "                         padding='VALID',\n",
    "                         name='pool1')\n",
    "  print_activations(pool1)\n",
    "\n",
    "  # conv2\n",
    "  with tf.name_scope('conv2') as scope:\n",
    "    kernel = tf.Variable(tf.truncated_normal([5, 5, 64, 192], dtype=dtype,\n",
    "                                             stddev=1e-1), name='weights')\n",
    "    conv = tf.nn.conv2d(pool1, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "    biases = tf.Variable(tf.constant(0.0, shape=[192], dtype=dtype),\n",
    "                         trainable=True, name='biases')\n",
    "    bias = tf.nn.bias_add(conv, biases)\n",
    "    conv2 = tf.nn.relu(bias, name=scope)\n",
    "    parameters += [kernel, biases]\n",
    "  print_activations(conv2)\n",
    "\n",
    "  ## lrn2\n",
    "  #with tf.name_scope('lrn2') as scope:\n",
    "    #lrn2 = tf.nn.local_response_normalization(conv2,\n",
    "                                              #alpha=1e-4,\n",
    "                                              #beta=0.75,\n",
    "                                              #depth_radius=2,\n",
    "                                              #bias=2.0)\n",
    "\n",
    "  # pool2\n",
    "  pool2 = tf.nn.max_pool(conv2,\n",
    "                         ksize=[1, 3, 3, 1],\n",
    "                         strides=[1, 2, 2, 1],\n",
    "                         padding='VALID',\n",
    "                         name='pool2')\n",
    "  print_activations(pool2)\n",
    "\n",
    "  # conv3\n",
    "  with tf.name_scope('conv3') as scope:\n",
    "    kernel = tf.Variable(tf.truncated_normal([3, 3, 192, 384],\n",
    "                                             dtype=dtype,\n",
    "                                             stddev=1e-1), name='weights')\n",
    "    conv = tf.nn.conv2d(pool2, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "    biases = tf.Variable(tf.constant(0.0, shape=[384], dtype=dtype),\n",
    "                         trainable=True, name='biases')\n",
    "    bias = tf.nn.bias_add(conv, biases)\n",
    "    conv3 = tf.nn.relu(bias, name=scope)\n",
    "    parameters += [kernel, biases]\n",
    "    print_activations(conv3)\n",
    "\n",
    "  # conv4\n",
    "  with tf.name_scope('conv4') as scope:\n",
    "    kernel = tf.Variable(tf.truncated_normal([3, 3, 384, 256],\n",
    "                                             dtype=dtype,\n",
    "                                             stddev=1e-1), name='weights')\n",
    "    conv = tf.nn.conv2d(conv3, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "    biases = tf.Variable(tf.constant(0.0, shape=[256], dtype=dtype),\n",
    "                         trainable=True, name='biases')\n",
    "    bias = tf.nn.bias_add(conv, biases)\n",
    "    conv4 = tf.nn.relu(bias, name=scope)\n",
    "    parameters += [kernel, biases]\n",
    "    print_activations(conv4)\n",
    "\n",
    "  # conv5\n",
    "  with tf.name_scope('conv5') as scope:\n",
    "    kernel = tf.Variable(tf.truncated_normal([3, 3, 256, 256],\n",
    "                                             dtype=dtype,\n",
    "                                             stddev=1e-1), name='weights')\n",
    "    conv = tf.nn.conv2d(conv4, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "    biases = tf.Variable(tf.constant(0.0, shape=[256], dtype=dtype),\n",
    "                         trainable=True, name='biases')\n",
    "    bias = tf.nn.bias_add(conv, biases)\n",
    "    conv5 = tf.nn.relu(bias, name=scope)\n",
    "    parameters += [kernel, biases]\n",
    "    print_activations(conv5)\n",
    "\n",
    "  # pool5\n",
    "  pool5 = tf.nn.max_pool(conv5,\n",
    "                         ksize=[1, 3, 3, 1],\n",
    "                         strides=[1, 2, 2, 1],\n",
    "                         padding='VALID',\n",
    "                         name='pool5')\n",
    "  print_activations(pool5)\n",
    "\n",
    "  return pool5, parameters\n",
    "\n",
    "\n",
    "def time_tensorflow_run(session, target, info_string):\n",
    "  \"\"\"Run the computation to obtain the target tensor and print timing stats.\n",
    "\n",
    "  Args:\n",
    "    session: the TensorFlow session to run the computation under.\n",
    "    target: the target Tensor that is passed to the session's run() function.\n",
    "    info_string: a string summarizing this run, to be printed with the stats.\n",
    "\n",
    "  Returns:\n",
    "    None\n",
    "  \"\"\"\n",
    "  num_steps_burn_in = 100\n",
    "  total_duration = 0.0\n",
    "  total_duration_squared = 0.0\n",
    "  for i in xrange(FLAGS.num_batches + num_steps_burn_in):\n",
    "    start_time = time.time()\n",
    "    _ = session.run(target)\n",
    "    duration = time.time() - start_time\n",
    "    if i >= num_steps_burn_in:\n",
    "      if not i % 100:\n",
    "        print ('%s: step %d, duration = %.3f' %\n",
    "               (datetime.now(), i - num_steps_burn_in, duration))\n",
    "      total_duration += duration\n",
    "      total_duration_squared += duration * duration\n",
    "  mn = total_duration / FLAGS.num_batches\n",
    "  vr = total_duration_squared / FLAGS.num_batches - mn * mn\n",
    "  sd = math.sqrt(vr)\n",
    "  print ('%s: %s across %d steps, %.3f +/- %.3f sec / batch' %\n",
    "         (datetime.now(), info_string, FLAGS.num_batches, mn, sd))\n",
    "\n",
    "\n",
    "\n",
    "def run_benchmark():\n",
    "  \"\"\"Run the benchmark on AlexNet.\"\"\"\n",
    "  dtype         = tf.float16\n",
    "  with tf.variable_scope('fp32_storage', custom_getter=float32_variable_storage_getter):\n",
    "\n",
    "\n",
    "        # Generate some dummy images.\n",
    "        image_size = 224\n",
    "        # Note that our padding definition is slightly different the cuda-convnet.\n",
    "        # In order to force the model to start with the same activations sizes,\n",
    "        # we add 3 to the image_size and employ VALID padding above.\n",
    "        images = tf.Variable(tf.random_normal([FLAGS.batch_size,\n",
    "                                            image_size,\n",
    "                                            image_size, 3],\n",
    "                                            dtype=dtype,\n",
    "                                            stddev=1e-1))\n",
    "\n",
    "        # Build a Graph that computes the logits predictions from the\n",
    "        # inference model.\n",
    "        pool5, parameters = inference(images, dtype)\n",
    "\n",
    "        # Build an initialization operation.\n",
    "        init = tf.global_variables_initializer()\n",
    "\n",
    "        # Start running operations on the Graph.\n",
    "        config = tf.ConfigProto()\n",
    "        config.gpu_options.allocator_type = 'BFC'\n",
    "        sess = tf.Session(config=config)\n",
    "        sess.run(init)\n",
    "\n",
    "        # Run the forward benchmark.\n",
    "        time_tensorflow_run(sess, pool5, \"Forward\")\n",
    "\n",
    "        # Add a simple objective so we can calculate the backward pass.\n",
    "        objective = tf.nn.l2_loss(pool5)\n",
    "        # Compute the gradient with respect to all the parameters.\n",
    "        grad = tf.gradients(objective, parameters)\n",
    "        # Run the backward benchmark.\n",
    "        time_tensorflow_run(sess, grad, \"Forward-backward\")\n",
    "\n",
    "\n",
    "def main(_):\n",
    "  run_benchmark()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  parser = argparse.ArgumentParser()\n",
    "  parser.add_argument(\n",
    "      '--batch_size',\n",
    "      type=int,\n",
    "      default=128,\n",
    "      help='Batch size.'\n",
    "  )\n",
    "  parser.add_argument(\n",
    "      '--num_batches',\n",
    "      type=int,\n",
    "      default=1000,\n",
    "      help='Number of batches to run.'\n",
    "  )\n",
    "  FLAGS, unparsed = parser.parse_known_args()\n",
    "  tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
