{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "Training for b=50\n",
      "Training for b=100\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import time\n",
    "import os\n",
    "from pathlib import Path\n",
    "import datetime\n",
    "import csv\n",
    "\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "epoch = datetime.datetime.utcfromtimestamp(0)\n",
    "\n",
    "def unix_time_secs(dt):\n",
    "    return round((dt - epoch).total_seconds())\n",
    "\n",
    "def weight_variable(shape,astype):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.01, dtype = astype)\n",
    "    return tf.Variable(initial, dtype = astype)\n",
    "\n",
    "def bias_variable(shape,astype):\n",
    "    initial = tf.constant(0.0, shape=shape, dtype = astype)\n",
    "    return tf.Variable(initial, dtype = astype)\n",
    "\n",
    "def nn_example(e, b, data_type):\n",
    "    learning_rate = 0.5\n",
    "    epochs = e\n",
    "    batch_size = b\n",
    "    input_feature_count = 784\n",
    "    out_classes = 10\n",
    "    data_type = data_type\n",
    "    \n",
    "    # Neural network hidden layer variables\n",
    "    h1 = 500\n",
    "    h2 = 200\n",
    "\n",
    "    # declare the training data placeholders\n",
    "    # input x - for 28 x 28 pixels = 784\n",
    "    x = tf.placeholder(data_type, [None, input_feature_count])\n",
    "    # now declare the output data placeholder - 10 digits\n",
    "    y = tf.placeholder(data_type, [None, out_classes])\n",
    "\n",
    "    # build the network\n",
    "    keep_prob_input = tf.placeholder(data_type)\n",
    "    x_drop = tf.nn.dropout(x, keep_prob=keep_prob_input)\n",
    "\n",
    "    W_fc1 = weight_variable([input_feature_count, h1], data_type)\n",
    "    b_fc1 = bias_variable([h1], data_type)\n",
    "    h_fc1 = tf.nn.relu(tf.matmul(x_drop, W_fc1) + b_fc1)\n",
    "\n",
    "    keep_prob = tf.placeholder(data_type)\n",
    "    h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "    W_fc2 = weight_variable([h1, h2],data_type)\n",
    "    b_fc2 = bias_variable([h2], data_type)\n",
    "    h_fc2 = tf.nn.relu(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)\n",
    "\n",
    "    h_fc2_drop = tf.nn.dropout(h_fc2, keep_prob)\n",
    "\n",
    "    W_fc3 = weight_variable([h2, out_classes],data_type)\n",
    "    b_fc3 = bias_variable([out_classes], data_type)\n",
    "\n",
    "    # now calculate the hidden layer output - in this case, let's use a softmax activated\n",
    "    # output layer\n",
    "    y_ = tf.nn.softmax(tf.matmul(h_fc2_drop, W_fc3) + b_fc3)\n",
    "\n",
    "    # now let's define the cost function which we are going to train the model on\n",
    "    y_clipped = tf.clip_by_value(y_, 1e-10, 0.9999999)\n",
    "    cross_entropy = -tf.reduce_mean(tf.reduce_sum(y * tf.log(y_clipped)\n",
    "                                                  + (1 - y) * tf.log(1 - y_clipped), axis=1))\n",
    "\n",
    "    # add an optimiser\n",
    "    optimiser = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cross_entropy)\n",
    "\n",
    "    # finally setup the initialisation operator\n",
    "    init_op = tf.global_variables_initializer()\n",
    "\n",
    "    # define an accuracy assessment operation\n",
    "    correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, data_type))\n",
    "\n",
    "    # add a summary to store the accuracy\n",
    "    accuracy_sum = tf.summary.scalar('accuracy', accuracy)\n",
    "    \n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    test_accuracy = 0\n",
    "    # start the session\n",
    "    with tf.Session() as sess:\n",
    "        # initialise the variables\n",
    "        sess.run(init_op)\n",
    "        total_batch = int(len(mnist.train.labels) / batch_size)\n",
    "        start_time = time.time()\n",
    "        for epoch in range(epochs):\n",
    "            avg_cost = 0\n",
    "            train_acc = 0\n",
    "            for i in range(total_batch):\n",
    "                batch_x, batch_y = mnist.train.next_batch(batch_size=batch_size)\n",
    "                train_acc,_, c = sess.run([accuracy, optimiser, cross_entropy], feed_dict={x: batch_x, y: batch_y, keep_prob_input: 1.0, keep_prob: 1.0})\n",
    "                avg_cost += c / total_batch\n",
    "            elapsed = time.time() - start_time\n",
    "            test_accuracy = sess.run(accuracy, feed_dict={x: mnist.test.images, y: mnist.test.labels, keep_prob_input: 1.0, keep_prob: 1.0})\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    data_type = tf.float16\n",
    "    filename = r'f16.csv'\n",
    "    file = open(filename, \"w\", newline='\\n')\n",
    "    batch_size = [50, 100, 200, 500, 1000, 2000, 5000, 10000, 20000, 30000, 40000, 50000, 55000]\n",
    "    epochs = 15\n",
    "    num_bsize = len(batch_size)\n",
    "    mat = np.zeros((num_bsize, 4))\n",
    "    start_time = 0\n",
    "    for i, b in enumerate(batch_size):\n",
    "        print('Training for b={}'.format(b))\n",
    "        start_time = datetime.datetime.now()\n",
    "        nn_example(epochs, b, data_type)\n",
    "        end_time = datetime.datetime.now()\n",
    "        line = str(b)+','+str(unix_time_secs(start_time))+','+str(unix_time_secs(end_time))+','+str(unix_time_secs(end_time)-unix_time_secs(start_time))\n",
    "        file.write(line)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    data_type = tf.float32\n",
    "    filename = r'f32.csv'\n",
    "    file = open(filename, \"w\", newline='\\n')\n",
    "    batch_size = [50, 100, 200, 500, 1000, 2000, 5000, 10000, 20000, 30000, 40000, 50000, 55000]\n",
    "    epochs = 15\n",
    "    num_bsize = len(batch_size)\n",
    "    mat = np.zeros((num_bsize, 4))\n",
    "    start_time = 0\n",
    "    for i, b in enumerate(batch_size):\n",
    "        print('Training for b={}'.format(b))\n",
    "        start_time = datetime.datetime.now()\n",
    "        nn_example(epochs, b, data_type)\n",
    "        end_time = datetime.datetime.now()\n",
    "        line = str(b)+','+str(unix_time_secs(start_time))+','+str(unix_time_secs(end_time))+','+str(unix_time_secs(end_time)-unix_time_secs(start_time))\n",
    "        file.write(line)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    data_type = tf.float64\n",
    "    filename = r'f64.csv'\n",
    "    file = open(filename, \"w\", newline='\\n')\n",
    "    batch_size = [50, 100, 200, 500, 1000, 2000, 5000, 10000, 20000, 30000, 40000, 50000, 55000]\n",
    "    epochs = 15\n",
    "    num_bsize = len(batch_size)\n",
    "    mat = np.zeros((num_bsize, 4))\n",
    "    start_time = 0\n",
    "    for i, b in enumerate(batch_size):\n",
    "        print('Training for b={}'.format(b))\n",
    "        start_time = datetime.datetime.now()\n",
    "        nn_example(epochs, b, data_type)\n",
    "        end_time = datetime.datetime.now()\n",
    "        line = str(b)+','+str(unix_time_secs(start_time))+','+str(unix_time_secs(end_time))+','+str(unix_time_secs(end_time)-unix_time_secs(start_time))\n",
    "        file.write(line)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nvidia-smi dmon --id GPU-335ff674-8b1f-c537-4fe9-9ec065706865 --delay 1 -s pucmt --options DT -f \"E:\\Work\\nvidia.txt\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
