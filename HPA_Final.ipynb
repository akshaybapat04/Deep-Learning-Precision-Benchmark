{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import time\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def weight_variable(shape,astype):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.01, dtype = astype)\n",
    "    return tf.Variable(initial, dtype = astype)\n",
    "\n",
    "def bias_variable(shape,astype):\n",
    "    initial = tf.constant(0.0, shape=shape, dtype = astype)\n",
    "    return tf.Variable(initial, dtype = astype)\n",
    "\n",
    "def nn_example(e, b, data_type, filepath):\n",
    "    learning_rate = 0.5\n",
    "    epochs = e\n",
    "    batch_size = b\n",
    "    input_feature_count = 784\n",
    "    out_classes = 10\n",
    "    data_type = data_type\n",
    "    \n",
    "    # Neural network hidden layer variables\n",
    "    h1 = 50\n",
    "    h2 = 20\n",
    "\n",
    "    # declare the training data placeholders\n",
    "    # input x - for 28 x 28 pixels = 784\n",
    "    x = tf.placeholder(data_type, [None, input_feature_count])\n",
    "    # now declare the output data placeholder - 10 digits\n",
    "    y = tf.placeholder(data_type, [None, out_classes])\n",
    "\n",
    "    # build the network\n",
    "    keep_prob_input = tf.placeholder(data_type)\n",
    "    x_drop = tf.nn.dropout(x, keep_prob=keep_prob_input)\n",
    "\n",
    "    W_fc1 = weight_variable([input_feature_count, h1], data_type)\n",
    "    b_fc1 = bias_variable([h1], data_type)\n",
    "    h_fc1 = tf.nn.relu(tf.matmul(x_drop, W_fc1) + b_fc1)\n",
    "\n",
    "    keep_prob = tf.placeholder(data_type)\n",
    "    h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "    W_fc2 = weight_variable([h1, h2],data_type)\n",
    "    b_fc2 = bias_variable([h2], data_type)\n",
    "    h_fc2 = tf.nn.relu(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)\n",
    "\n",
    "    h_fc2_drop = tf.nn.dropout(h_fc2, keep_prob)\n",
    "\n",
    "    W_fc3 = weight_variable([h2, out_classes],data_type)\n",
    "    b_fc3 = bias_variable([out_classes], data_type)\n",
    "\n",
    "    # now calculate the hidden layer output - in this case, let's use a softmax activated\n",
    "    # output layer\n",
    "    y_ = tf.nn.softmax(tf.matmul(h_fc2_drop, W_fc3) + b_fc3)\n",
    "\n",
    "    # now let's define the cost function which we are going to train the model on\n",
    "    y_clipped = tf.clip_by_value(y_, 1e-10, 0.9999999)\n",
    "    cross_entropy = -tf.reduce_mean(tf.reduce_sum(y * tf.log(y_clipped)\n",
    "                                                  + (1 - y) * tf.log(1 - y_clipped), axis=1))\n",
    "\n",
    "    # add an optimiser\n",
    "    optimiser = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cross_entropy)\n",
    "\n",
    "    # finally setup the initialisation operator\n",
    "    init_op = tf.global_variables_initializer()\n",
    "\n",
    "    # define an accuracy assessment operation\n",
    "    correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, data_type))\n",
    "\n",
    "    # add a summary to store the accuracy\n",
    "    accuracy_sum = tf.summary.scalar('accuracy', accuracy)\n",
    "    \n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    merged = tf.summary.merge([accuracy_sum])\n",
    "    writer = tf.summary.FileWriter(r'C:\\Users\\anant\\dev\\repos\\HPA')\n",
    "    \n",
    "    test_accuracy = 0\n",
    "    # start the session\n",
    "    with tf.Session() as sess:\n",
    "        # initialise the variables\n",
    "        sess.run(init_op)\n",
    "        total_batch = int(len(mnist.train.labels) / batch_size)\n",
    "        start_time = time.time()\n",
    "        for epoch in range(max(epochs)+1):\n",
    "            avg_cost = 0\n",
    "            train_acc = 0\n",
    "            for i in range(total_batch):\n",
    "                batch_x, batch_y = mnist.train.next_batch(batch_size=batch_size)\n",
    "                train_acc,_, c = sess.run([accuracy, optimiser, cross_entropy], feed_dict={x: batch_x, y: batch_y, keep_prob_input: 1.0, keep_prob: 1.0})\n",
    "                avg_cost += c / total_batch\n",
    "                arr_accuracy = [];\n",
    "                time_elapsed = [];\n",
    "                nw_size = [];\n",
    "            if(epoch in epochs):\n",
    "                elapsed = time.time() - start_time\n",
    "                filepath = \"D:\\\\UCI\\\\HPA_Project\\\\weights_data\\\\epoch_\"+ str(epoch)+ '_batch_' +str(batch_size)\n",
    "                save_path = saver.save(sess, filepath)\n",
    "                mfilepath = filepath + \".data-00000-of-00001\"\n",
    "                file = Path(mfilepath)\n",
    "                size = file.stat().st_size\n",
    "                test_accuracy = sess.run(accuracy, feed_dict={x: mnist.test.images, y: mnist.test.labels, keep_prob_input: 1.0, keep_prob: 1.0})\n",
    "                arr_accuracy.append(test_accuracy)\n",
    "                time_elapsed.append(elapsed)\n",
    "                nw_size.append(size)\n",
    "                print(epoch, b, test_accuracy, elapsed)\n",
    "    return arr_accuracy, time_elapsed, nw_size\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    data_type = tf.float32\n",
    "    batch_size = [50, 100, 200, 500, 1000]\n",
    "    epochs = [5,10, 15, 20] #, 30, 50] #, 80, 100, 150, 200, 300, 500, 700, 1000]\n",
    "    num_epochs = len(epochs)\n",
    "    num_bsize = len(batch_size)\n",
    "    accuracy_mat = [[] for i in range(num_bsize)]\n",
    "    elapsed_time = [[] for i in range(num_bsize)]\n",
    "    size_mat = [[] for i in range(num_bsize)]\n",
    "    for j, b in enumerate(batch_size):\n",
    "        accuracy_mat[j], elapsed_time[j], size_mat[j] = nn_example(epochs, b, data_type, filepath)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = [20, 30, 50, 80, 100, 150, 200, 300, 500, 700, 1000]\n",
    "for epoch in range(max(epochs)+1):\n",
    "    if(epoch in epochs):\n",
    "        print(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import time\n",
    "import os\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def weight_variable(shape,astype):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.01, dtype = astype)\n",
    "    return tf.Variable(initial, dtype = astype)\n",
    "\n",
    "def bias_variable(shape,astype):\n",
    "    initial = tf.constant(0.0, shape=shape, dtype = astype)\n",
    "    return tf.Variable(initial, dtype = astype)\n",
    "\n",
    "def nn_example(e, batch, data_type, filepath):\n",
    "    # Python optimisation variables\n",
    "    learning_rate = 0.5\n",
    "    epochs = e\n",
    "    batch_size = b\n",
    "    input_feature_count = 784\n",
    "    out_classes = 10\n",
    "    data_type = data_type\n",
    "    \n",
    "    # Neural network hidden layer variables\n",
    "    h1 = 50\n",
    "    h2 = 20\n",
    "\n",
    "    # declare the training data placeholders\n",
    "    # input x - for 28 x 28 pixels = 784\n",
    "    x = tf.placeholder(data_type, [None, input_feature_count])\n",
    "    # now declare the output data placeholder - 10 digits\n",
    "    y = tf.placeholder(data_type, [None, out_classes])\n",
    "\n",
    "    # build the network\n",
    "    keep_prob_input = tf.placeholder(data_type)\n",
    "    x_drop = tf.nn.dropout(x, keep_prob=keep_prob_input)\n",
    "\n",
    "    W_fc1 = weight_variable([input_feature_count, h1], data_type)\n",
    "    b_fc1 = bias_variable([h1], data_type)\n",
    "    h_fc1 = tf.nn.relu(tf.matmul(x_drop, W_fc1) + b_fc1)\n",
    "\n",
    "    keep_prob = tf.placeholder(data_type)\n",
    "    h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "    W_fc2 = weight_variable([h1, h2],data_type)\n",
    "    b_fc2 = bias_variable([h2], data_type)\n",
    "    h_fc2 = tf.nn.relu(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)\n",
    "\n",
    "    h_fc2_drop = tf.nn.dropout(h_fc2, keep_prob)\n",
    "\n",
    "    W_fc3 = weight_variable([h2, out_classes],data_type)\n",
    "    b_fc3 = bias_variable([out_classes], data_type)\n",
    "\n",
    "    # now calculate the hidden layer output - in this case, let's use a softmax activated\n",
    "    # output layer\n",
    "    y_ = tf.nn.softmax(tf.matmul(h_fc2_drop, W_fc3) + b_fc3)\n",
    "\n",
    "    # now let's define the cost function which we are going to train the model on\n",
    "    y_clipped = tf.clip_by_value(y_, 1e-10, 0.9999999)\n",
    "    cross_entropy = -tf.reduce_mean(tf.reduce_sum(y * tf.log(y_clipped)\n",
    "                                                  + (1 - y) * tf.log(1 - y_clipped), axis=1))\n",
    "\n",
    "    # add an optimiser\n",
    "    optimiser = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cross_entropy)\n",
    "\n",
    "    # finally setup the initialisation operator\n",
    "    init_op = tf.global_variables_initializer()\n",
    "\n",
    "    # define an accuracy assessment operation\n",
    "    correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, data_type))\n",
    "\n",
    "    # add a summary to store the accuracy\n",
    "    accuracy_sum = tf.summary.scalar('accuracy', accuracy)\n",
    "    \n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    merged = tf.summary.merge([accuracy_sum])\n",
    "    writer = tf.summary.FileWriter(r'C:\\Users\\anant\\dev\\repos\\HPA')\n",
    "    \n",
    "    test_accuracy = 0\n",
    "    # start the session\n",
    "    with tf.Session() as sess:\n",
    "        # initialise the variables\n",
    "        sess.run(init_op)\n",
    "        total_batch = int(len(mnist.train.labels) / batch_size)\n",
    "        start_time = time.time()\n",
    "        for epoch in range(epochs):\n",
    "            avg_cost = 0\n",
    "            train_acc = 0\n",
    "            for i in range(total_batch):\n",
    "                batch_x, batch_y = mnist.train.next_batch(batch_size=batch_size)\n",
    "                train_acc,_, c = sess.run([accuracy, optimiser, cross_entropy], feed_dict={x: batch_x, y: batch_y, keep_prob_input: 1.0, keep_prob: 1.0})\n",
    "                avg_cost += c / total_batch\n",
    "#             print(\"Epoch:\", (epoch + 1), \"Train accuracy =\", \"{:.3f}\".format(train_acc), \"cost =\", \"{:.3f}\".format(avg_cost))\n",
    "#             summary = sess.run(merged, feed_dict={x: mnist.test.images, y: mnist.test.labels, keep_prob_input: 1.0, keep_prob: 1.0})\n",
    "#             writer.add_summary(summary, epoch)\n",
    "        elapsed = time.time() - start_time\n",
    "#         print(\"\\nTraining complete!\")\n",
    "#         writer.add_graph(sess.graph)\n",
    "        save_path = saver.save(sess, filepath)\n",
    "        mfilepath = filepath + \".data-00000-of-00001\"\n",
    "        file = Path(mfilepath)\n",
    "        size = file.stat().st_size\n",
    "        test_accuracy = sess.run(accuracy, feed_dict={x: mnist.test.images, y: mnist.test.labels, keep_prob_input: 1.0, keep_prob: 1.0})\n",
    "#         print('For e={} & b={}, test_accuracy={}'.format(epoch, batch_size, test_accuracy))\n",
    "    return test_accuracy, elapsed, size\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    data_type = tf.float16\n",
    "    epochs = [5, 10, 15, 20, 50]\n",
    "#     epochs = [50]\n",
    "    batch_size = [50, 100, 200, 500, 1000]\n",
    "    \n",
    "    num_epochs = len(epochs)\n",
    "    num_bsize = len(batch_size)\n",
    "    accuracy_mat = [[0 for j in range(num_bsize)] for i in range(num_epochs)]\n",
    "    elapsed_time = [[0 for j in range(num_bsize)] for i in range(num_epochs)]\n",
    "    size_mat = [[0 for j in range(num_bsize)] for i in range(num_epochs)]\n",
    "    for i, e in enumerate(epochs):\n",
    "        for j, b in enumerate(batch_size):\n",
    "            filepath = \"D:\\\\UCI\\\\HPA_Project\\\\weights_data\\\\epoch_\"+ str(e)+ '_batch_' +str(b)\n",
    "            accuracy_mat[i][j], elapsed_time[i][j], size_mat[i][j]= nn_example(e, b, data_type, filepath)\n",
    "            print(epochs[i],batch_size[j],accuracy_mat[i][j], elapsed_time[i][j])\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "5 50 0.098022 28.80020833015442\n",
    "5 100 0.098022 16.516690492630005\n",
    "5 200 0.098022 10.767619371414185\n",
    "5 500 0.098022 6.33238673210144\n",
    "5 1000 0.098022 4.642341375350952\n",
    "\n",
    "10 50 0.098022 63.0987548828125\n",
    "10 100 0.098022 32.40916156768799\n",
    "10 200 0.098022 21.14822745323181\n",
    "10 500 0.098022 11.47575831413269\n",
    "10 1000 0.098022 10.29037618637085\n",
    "\n",
    "15 50 0.098022 87.81630396842957\n",
    "15 100 0.098022 48.233941316604614\n",
    "15 200 0.098022 28.5248761177063\n",
    "15 500 0.098022 16.72200894355774\n",
    "15 1000 0.098022 13.220077276229858"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_variable(shape,astype):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.01, dtype = astype)\n",
    "    return tf.Variable(initial, dtype = astype)\n",
    "\n",
    "def bias_variable(shape,astype):\n",
    "    initial = tf.constant(0.0, shape=shape, dtype = astype)\n",
    "    return tf.Variable(initial, dtype = astype)\n",
    "\n",
    "def nn_example(e, batch):\n",
    "    # Python optimisation variables\n",
    "    learning_rate = 0.5\n",
    "    epochs = e\n",
    "    batch_size = b\n",
    "    input_feature_count = 784\n",
    "    out_classes = 10\n",
    "    \n",
    "    # Neural network hidden layer variables\n",
    "    h1 = 50\n",
    "    h2 = 20\n",
    "\n",
    "    # declare the training data placeholders\n",
    "    # input x - for 28 x 28 pixels = 784\n",
    "    x = tf.placeholder(tf.float64, [None, input_feature_count])\n",
    "    # now declare the output data placeholder - 10 digits\n",
    "    y = tf.placeholder(tf.float64, [None, out_classes])\n",
    "\n",
    "    # build the network\n",
    "    keep_prob_input = tf.placeholder(tf.float64)\n",
    "    x_drop = tf.nn.dropout(x, keep_prob=keep_prob_input)\n",
    "\n",
    "    W_fc1 = weight_variable([input_feature_count, h1], tf.float64)\n",
    "    b_fc1 = bias_variable([h1], tf.float64)\n",
    "    h_fc1 = tf.nn.relu(tf.matmul(x_drop, W_fc1) + b_fc1)\n",
    "\n",
    "    keep_prob = tf.placeholder(tf.float64)\n",
    "    h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "    W_fc2 = weight_variable([h1, h2],tf.float64)\n",
    "    b_fc2 = bias_variable([h2], tf.float64)\n",
    "    h_fc2 = tf.nn.relu(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)\n",
    "\n",
    "    h_fc2_drop = tf.nn.dropout(h_fc2, keep_prob)\n",
    "\n",
    "    W_fc3 = weight_variable([h2, out_classes],tf.float64)\n",
    "    b_fc3 = bias_variable([out_classes], tf.float64)\n",
    "\n",
    "    # now calculate the hidden layer output - in this case, let's use a softmax activated\n",
    "    # output layer\n",
    "    y_ = tf.nn.softmax(tf.matmul(h_fc2_drop, W_fc3) + b_fc3)\n",
    "\n",
    "    # now let's define the cost function which we are going to train the model on\n",
    "    y_clipped = tf.clip_by_value(y_, 1e-10, 0.9999999)\n",
    "    cross_entropy = -tf.reduce_mean(tf.reduce_sum(y * tf.log(y_clipped)\n",
    "                                                  + (1 - y) * tf.log(1 - y_clipped), axis=1))\n",
    "\n",
    "    # add an optimiser\n",
    "    optimiser = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cross_entropy)\n",
    "\n",
    "    # finally setup the initialisation operator\n",
    "    init_op = tf.global_variables_initializer()\n",
    "\n",
    "    # define an accuracy assessment operation\n",
    "    correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float64))\n",
    "\n",
    "    # add a summary to store the accuracy\n",
    "    accuracy_sum = tf.summary.scalar('accuracy', accuracy)\n",
    "\n",
    "    merged = tf.summary.merge([accuracy_sum])\n",
    "    writer = tf.summary.FileWriter(r'C:\\Users\\anant\\dev\\repos\\HPA')\n",
    "    \n",
    "    test_accuracy = 0\n",
    "    # start the session\n",
    "    with tf.Session() as sess:\n",
    "        # initialise the variables\n",
    "        sess.run(init_op)\n",
    "        total_batch = int(len(mnist.train.labels) / batch_size)\n",
    "        start_time = time.time()\n",
    "        for epoch in range(epochs):\n",
    "            avg_cost = 0\n",
    "            train_acc = 0\n",
    "            for i in range(total_batch):\n",
    "                batch_x, batch_y = mnist.train.next_batch(batch_size=batch_size)\n",
    "                train_acc,_, c = sess.run([accuracy, optimiser, cross_entropy], feed_dict={x: batch_x, y: batch_y, keep_prob_input: 1.0, keep_prob: 1.0})\n",
    "                avg_cost += c / total_batch\n",
    "#             print(\"Epoch:\", (epoch + 1), \"Train accuracy =\", \"{:.3f}\".format(train_acc), \"cost =\", \"{:.3f}\".format(avg_cost))\n",
    "#             summary = sess.run(merged, feed_dict={x: mnist.test.images, y: mnist.test.labels, keep_prob_input: 1.0, keep_prob: 1.0})\n",
    "#             writer.add_summary(summary, epoch)\n",
    "        elapsed = time.time() - start_time\n",
    "#         print(\"\\nTraining complete!\")\n",
    "#         writer.add_graph(sess.graph)\n",
    "        test_accuracy = sess.run(accuracy, feed_dict={x: mnist.test.images, y: mnist.test.labels, keep_prob_input: 1.0, keep_prob: 1.0})\n",
    "#         print('For e={} & b={}, test_accuracy={}'.format(epoch, batch_size, test_accuracy))\n",
    "    return test_accuracy, elapsed\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    epochs = [1, 2, 3, 5, 10]\n",
    "    batch_size = [10, 20, 50, 100, 200, 500, 1000]\n",
    "    \n",
    "    num_epochs = len(epochs)\n",
    "    num_bsize = len(batch_size)\n",
    "    accuracy_mat = [[0 for j in range(num_bsize)] for i in range(num_epochs)]\n",
    "    elapsed_time = [[0 for j in range(num_bsize)] for i in range(num_epochs)]\n",
    "    for i, e in enumerate(epochs):\n",
    "        for j, b in enumerate(batch_size):\n",
    "            accuracy_mat[i][j], elapsed_time[i][j]= nn_example(e, b)\n",
    "            print(accuracy_mat[i][j], elapsed_time[i][j])\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_variable(shape,astype):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.01, dtype = astype)\n",
    "    return tf.Variable(initial, dtype = astype)\n",
    "\n",
    "def bias_variable(shape,astype):\n",
    "    initial = tf.constant(0.0, shape=shape, dtype = astype)\n",
    "    return tf.Variable(initial, dtype = astype)\n",
    "\n",
    "def nn_example(e, batch):\n",
    "    # Python optimisation variables\n",
    "    learning_rate = 0.5\n",
    "    epochs = e\n",
    "    batch_size = b\n",
    "    input_feature_count = 784\n",
    "    out_classes = 10\n",
    "    \n",
    "    # Neural network hidden layer variables\n",
    "    h1 = 50\n",
    "    h2 = 20\n",
    "\n",
    "    # declare the training data placeholders\n",
    "    # input x - for 28 x 28 pixels = 784\n",
    "    x = tf.placeholder(tf.float16, [None, input_feature_count])\n",
    "    # now declare the output data placeholder - 10 digits\n",
    "    y = tf.placeholder(tf.float16, [None, out_classes])\n",
    "\n",
    "    # build the network\n",
    "    keep_prob_input = tf.placeholder(tf.float16)\n",
    "    x_drop = tf.nn.dropout(x, keep_prob=keep_prob_input)\n",
    "\n",
    "    W_fc1 = weight_variable([input_feature_count, h1], tf.float16)\n",
    "    b_fc1 = bias_variable([h1], tf.float16)\n",
    "    h_fc1 = tf.nn.relu(tf.matmul(x_drop, W_fc1) + b_fc1)\n",
    "\n",
    "    keep_prob = tf.placeholder(tf.float16)\n",
    "    h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "    W_fc2 = weight_variable([h1, h2],tf.float16)\n",
    "    b_fc2 = bias_variable([h2], tf.float16)\n",
    "    h_fc2 = tf.nn.relu(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)\n",
    "\n",
    "    h_fc2_drop = tf.nn.dropout(h_fc2, keep_prob)\n",
    "\n",
    "    W_fc3 = weight_variable([h2, out_classes],tf.float16)\n",
    "    b_fc3 = bias_variable([out_classes], tf.float16)\n",
    "\n",
    "    # now calculate the hidden layer output - in this case, let's use a softmax activated\n",
    "    # output layer\n",
    "    y_ = tf.nn.softmax(tf.matmul(h_fc2_drop, W_fc3) + b_fc3)\n",
    "\n",
    "    # now let's define the cost function which we are going to train the model on\n",
    "    y_clipped = tf.clip_by_value(y_, 1e-10, 0.9999999)\n",
    "    cross_entropy = -tf.reduce_mean(tf.reduce_sum(y * tf.log(y_clipped)\n",
    "                                                  + (1 - y) * tf.log(1 - y_clipped), axis=1))\n",
    "\n",
    "    # add an optimiser\n",
    "    optimiser = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cross_entropy)\n",
    "\n",
    "    # finally setup the initialisation operator\n",
    "    init_op = tf.global_variables_initializer()\n",
    "\n",
    "    # define an accuracy assessment operation\n",
    "    correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float16))\n",
    "\n",
    "    # add a summary to store the accuracy\n",
    "    accuracy_sum = tf.summary.scalar('accuracy', accuracy)\n",
    "\n",
    "    merged = tf.summary.merge([accuracy_sum])\n",
    "    writer = tf.summary.FileWriter(r'C:\\Users\\anant\\dev\\repos\\HPA')\n",
    "    \n",
    "    test_accuracy = 0\n",
    "    # start the session\n",
    "    with tf.Session() as sess:\n",
    "        # initialise the variables\n",
    "        sess.run(init_op)\n",
    "        total_batch = int(len(mnist.train.labels) / batch_size)\n",
    "        start_time = time.time()\n",
    "        for epoch in range(epochs):\n",
    "            avg_cost = 0\n",
    "            train_acc = 0\n",
    "            for i in range(total_batch):\n",
    "                batch_x, batch_y = mnist.train.next_batch(batch_size=batch_size)\n",
    "                train_acc,_, c = sess.run([accuracy, optimiser, cross_entropy], feed_dict={x: batch_x, y: batch_y, keep_prob_input: 1.0, keep_prob: 1.0})\n",
    "                avg_cost += c / total_batch\n",
    "#             print(\"Epoch:\", (epoch + 1), \"Train accuracy =\", \"{:.3f}\".format(train_acc), \"cost =\", \"{:.3f}\".format(avg_cost))\n",
    "#             summary = sess.run(merged, feed_dict={x: mnist.test.images, y: mnist.test.labels, keep_prob_input: 1.0, keep_prob: 1.0})\n",
    "#             writer.add_summary(summary, epoch)\n",
    "        elapsed = time.time() - start_time\n",
    "#         print(\"\\nTraining complete!\")\n",
    "#         writer.add_graph(sess.graph)\n",
    "        test_accuracy = sess.run(accuracy, feed_dict={x: mnist.test.images, y: mnist.test.labels, keep_prob_input: 1.0, keep_prob: 1.0})\n",
    "#         print('For e={} & b={}, test_accuracy={}'.format(epoch, batch_size, test_accuracy))\n",
    "    return test_accuracy, elapsed\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    epochs = [1, 2, 3, 5, 10]\n",
    "    batch_size = [10, 20, 50, 100, 200, 500, 1000]\n",
    "    \n",
    "    num_epochs = len(epochs)\n",
    "    num_bsize = len(batch_size)\n",
    "    accuracy_mat = [[0 for j in range(num_bsize)] for i in range(num_epochs)]\n",
    "    elapsed_time = [[0 for j in range(num_bsize)] for i in range(num_epochs)]\n",
    "    for i, e in enumerate(epochs):\n",
    "        for j, b in enumerate(batch_size):\n",
    "            accuracy_mat[i][j], elapsed_time[i][j]= nn_example(e, b)\n",
    "            print(accuracy_mat[i][j], elapsed_time[i][j])\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "%matplotlib inline\n",
    "sys.path.append(r'D:\\UCI\\CS273A-ML\\HW4-code') #append path for mltools\n",
    "sys.path.append(r'D:\\UCI\\CS273A-ML\\Project') #append path for data\n",
    "np.random.seed(0)\n",
    "\n",
    "import mltools as ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    # Data Loading\n",
    "    X = np.genfromtxt(r'D:\\UCI\\CS273A-ML\\Project\\X_train.txt' , delimiter=None)\n",
    "    Y = np.genfromtxt(r'D:\\UCI\\CS273A-ML\\Project\\Y_train.txt' , delimiter=None)\n",
    "    X, Y = ml.shuffleData(X,Y)\n",
    "    \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X, Y = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_ml_hw(X, Y):\n",
    "    onehot_encoder = OneHotEncoder(sparse=False)\n",
    "    Xtr, Xva, Ytr, Yva = ml.splitData(X, Y)\n",
    "    Ytr = onehot_encoder.fit_transform(Ytr.reshape(len(Ytr), 1))\n",
    "    Yva = onehot_encoder.fit_transform(Yva.reshape(len(Yva), 1))\n",
    "    Xt, Yt = Xtr[:], Ytr[:] # subsample for efficiency (you can go higher)\n",
    "    Xv, Yv = Xva[:], Yva[:] # subsample for efficiency (you can go higher)\n",
    "    Xts, params = ml.rescale(Xt) # Normalize the features\n",
    "#     Xvs, _ = ml.rescale(Xv, params) # Normalize the features\n",
    "    print(Xts.shape, Yt.shape, Xv.shape, Yv.shape)\n",
    "    return Xt, Yt, Xv, Yv\n",
    "    \n",
    "read_ml_hw(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import pickle\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def weight_variable(shape,astype):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.01, dtype = astype)\n",
    "    return tf.Variable(initial, dtype = astype)\n",
    "\n",
    "\n",
    "def bias_variable(shape,astype):\n",
    "    initial = tf.constant(0.0, shape=shape, dtype = astype)\n",
    "    return tf.Variable(initial, dtype = astype)\n",
    "\n",
    "def read_input():\n",
    "    inp_file = open(r'C:\\Users\\anant\\dev\\repos\\HPA\\inputs', 'rb')\n",
    "    inp = pickle.load(inp_file)\n",
    "    inp_file.close()\n",
    "    print(inp[:3])\n",
    "    inp[:,0:2] = inp[:,0:2]*10\n",
    "    print(inp[:3])\n",
    "    \n",
    "    out_file = open(r'C:\\Users\\anant\\dev\\repos\\HPA\\outputs', 'rb')\n",
    "    out = pickle.load(out_file)\n",
    "    out_file.close()\n",
    "\n",
    "    onehot_encoder = OneHotEncoder(sparse=False)\n",
    "    onehot_encoded = onehot_encoder.fit_transform(out.reshape(len(out), 1))\n",
    "\n",
    "    inp_train, inp_val, onehot_encoded_train, onehot_encoded_val = train_test_split(inp, onehot_encoded)\n",
    "    print(inp_train.shape, onehot_encoded_train.shape, inp_val.shape, onehot_encoded_val.shape)\n",
    "    return inp_train, onehot_encoded_train, inp_val, onehot_encoded_val\n",
    "\n",
    "def read_ml_hw(X, Y):\n",
    "    onehot_encoder = OneHotEncoder(sparse=False)\n",
    "    Xtr, Xva, Ytr, Yva = ml.splitData(X, Y)\n",
    "    Ytr = onehot_encoder.fit_transform(Ytr.reshape(len(Ytr), 1))\n",
    "    Yva = onehot_encoder.fit_transform(Yva.reshape(len(Yva), 1))\n",
    "    Xt, Yt = Xtr[:], Ytr[:] # subsample for efficiency (you can go higher)\n",
    "    Xv, Yv = Xva[:], Yva[:] # subsample for efficiency (you can go higher)\n",
    "    Xts, params = ml.rescale(Xt) # Normalize the features\n",
    "#     Xvs, _ = ml.rescale(Xv, params) # Normalize the features\n",
    "    print(Xts.shape, Yt.shape, Xv.shape, Yv.shape)\n",
    "    return Xt, Yt, Xv, Yv\n",
    "\n",
    "def nn_example():\n",
    "#     inp, out, inp_va, out_va = read_input()\n",
    "    X, Y = load_data()\n",
    "    inp, out, inp_va, out_va = read_ml_hw(X, Y)\n",
    "    no_of_data = inp.shape[0]\n",
    "    input_feature_count = inp.shape[1]\n",
    "    out_classes = out.shape[1]\n",
    "    print('Features={}, classes={}'.format(input_feature_count, out_classes))\n",
    "    print('Inp_train={}, inp_val={}, out_train={}, out_val={}'.format(inp.shape, inp_va.shape, out.shape, out_va.shape))\n",
    "    \n",
    "    # Python optimisation variables\n",
    "    learning_rate = 0.001\n",
    "    epochs = 100\n",
    "    batch_size = 1\n",
    "    \n",
    "    # Neural network hidden layer variables\n",
    "    h1 = 5\n",
    "    h2 = 3\n",
    "\n",
    "    # declare the training data placeholders\n",
    "    # input x - for 28 x 28 pixels = 784\n",
    "    x = tf.placeholder(tf.float32, [None, input_feature_count])\n",
    "    # now declare the output data placeholder - 10 digits\n",
    "    y = tf.placeholder(tf.float32, [None, out_classes])\n",
    "\n",
    "    # build the network\n",
    "    keep_prob_input = tf.placeholder(tf.float32)\n",
    "    x_drop = tf.nn.dropout(x, keep_prob=keep_prob_input)\n",
    "\n",
    "    W_fc1 = weight_variable([input_feature_count, h1], tf.float32)\n",
    "    b_fc1 = bias_variable([h1], tf.float32)\n",
    "    h_fc1 = tf.nn.relu(tf.matmul(x_drop, W_fc1) + b_fc1)\n",
    "\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "    h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "    W_fc2 = weight_variable([h1, h2],tf.float32)\n",
    "    b_fc2 = bias_variable([h2], tf.float32)\n",
    "    h_fc2 = tf.nn.relu(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)\n",
    "\n",
    "    h_fc2_drop = tf.nn.dropout(h_fc2, keep_prob)\n",
    "\n",
    "    W_fc3 = weight_variable([h2, out_classes],tf.float32)\n",
    "    b_fc3 = bias_variable([out_classes], tf.float32)\n",
    "\n",
    "    # now calculate the hidden layer output - in this case, let's use a softmax activated\n",
    "    # output layer\n",
    "    y_ = tf.nn.softmax(tf.matmul(h_fc2_drop, W_fc3) + b_fc3)\n",
    "\n",
    "    # now let's define the cost function which we are going to train the model on\n",
    "    y_clipped = tf.clip_by_value(y_, 1e-10, 0.9999999)\n",
    "    cross_entropy = -tf.reduce_mean(tf.reduce_sum(y * tf.log(y_clipped)\n",
    "                                                  + (1 - y) * tf.log(1 - y_clipped), axis=1))\n",
    "\n",
    "    # add an optimiser\n",
    "    optimiser = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cross_entropy)\n",
    "\n",
    "    # finally setup the initialisation operator\n",
    "    init_op = tf.global_variables_initializer()\n",
    "\n",
    "    # define an accuracy assessment operation\n",
    "    correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "    # add a summary to store the accuracy\n",
    "    accuracy_sum = tf.summary.scalar('accuracy', accuracy)\n",
    "\n",
    "    merged = tf.summary.merge([accuracy_sum])\n",
    "    writer = tf.summary.FileWriter(r'C:\\Users\\anant\\dev\\repos\\HPA')\n",
    "    # start the session\n",
    "    with tf.Session() as sess:\n",
    "        # initialise the variables\n",
    "        sess.run(init_op)\n",
    "        total_batch = no_of_data//batch_size\n",
    "        print('Total number of batches to train={}'.format(total_batch))\n",
    "        for epoch in range(epochs):\n",
    "            avg_cost = 0\n",
    "            train_acc = 0\n",
    "            for i in range(total_batch):\n",
    "                batch_x, batch_y = inp[i*batch_size:(i+1)*batch_size], out[i*batch_size:(i+1)*batch_size]\n",
    "                train_acc, _, c = sess.run([accuracy, optimiser, cross_entropy], feed_dict={x: batch_x, y: batch_y, keep_prob_input: 1.0, keep_prob: 1.0})\n",
    "                avg_cost += c / total_batch\n",
    "            print(\"Epoch:\", (epoch + 1), \"Train accuracy =\", \"{:.3f}\".format(train_acc), \"cost =\", \"{:.3f}\".format(avg_cost))\n",
    "#             summary = sess.run(merged, feed_dict={x: mnist.test.images, y: mnist.test.labels, keep_prob_input: 1.0, keep_prob: 1.0})\n",
    "#             writer.add_summary(summary, epoch)\n",
    "\n",
    "        print(\"\\nTraining complete!\")\n",
    "#         writer.add_graph(sess.graph)\n",
    "        print(sess.run(accuracy, feed_dict={x: inp_va, y: out_va, keep_prob_input: 1.0, keep_prob: 1.0}))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    nn_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Epoch: 1 Train accuracy = 0.650 cost = 1.249\n",
    "Epoch: 2 Train accuracy = 0.660 cost = 1.231\n",
    "Epoch: 3 Train accuracy = 0.680 cost = 1.227\n",
    "Epoch: 4 Train accuracy = 0.690 cost = 1.225\n",
    "Epoch: 5 Train accuracy = 0.670 cost = 1.223\n",
    "Epoch: 6 Train accuracy = 0.670 cost = 1.221\n",
    "Epoch: 7 Train accuracy = 0.670 cost = 1.220\n",
    "Epoch: 8 Train accuracy = 0.670 cost = 1.219\n",
    "Epoch: 9 Train accuracy = 0.690 cost = 1.218\n",
    "Epoch: 10 Train accuracy = 0.660 cost = 1.217\n",
    "\n",
    "Training complete!\n",
    "0.692975"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Below cell commented out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "\n",
    "%matplotlib inline\n",
    "sys.path.append(r'D:\\UCI\\CS273A-ML\\HW4-code') #append path for mltools\n",
    "sys.path.append(r'D:\\UCI\\CS273A-ML\\Project') #append path for data\n",
    "np.random.seed(0)\n",
    "\n",
    "import mltools as ml\n",
    "\n",
    "# Data Loading\n",
    "X = np.genfromtxt(r'D:\\UCI\\CS273A-ML\\Project\\X_train.txt' , delimiter=None)\n",
    "Y = np.genfromtxt(r'D:\\UCI\\CS273A-ML\\Project\\Y_train.txt' , delimiter=None)\n",
    "X, Y = ml.shuffleData(X,Y)\n",
    "\n",
    "Xtr, Xva, Ytr, Yva = ml.splitData(X, Y)\n",
    "Xt, Yt = Xtr[:5000], Ytr[:5000] # subsample for efficiency (you can go higher)\n",
    "Xv, Yv = Xva[:5000], Yva[:5000] # subsample for efficiency (you can go higher)\n",
    "Xts, params = ml.rescale(Xt) # Normalize the features\n",
    "Xvs, _ = ml.rescale(Xv, params) # Normalize the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_input():\n",
    "    inp_file = open(r'C:\\Users\\anant\\dev\\repos\\HPA\\inputs', 'rb')\n",
    "    inp = pickle.load(inp_file)\n",
    "    inp_file.close()\n",
    "    print(inp[:3])\n",
    "    inp[:,0:2] = inp[:,0:2]*10\n",
    "    print(inp[:3])\n",
    "    \n",
    "    out_file = open(r'C:\\Users\\anant\\dev\\repos\\HPA\\outputs', 'rb')\n",
    "    out = pickle.load(out_file)\n",
    "    out_file.close()\n",
    "\n",
    "    onehot_encoder = OneHotEncoder(sparse=False)\n",
    "    onehot_encoded = onehot_encoder.fit_transform(out.reshape(len(out), 1))\n",
    "\n",
    "    inp_train, inp_val, onehot_encoded_train, onehot_encoded_val = train_test_split(inp, onehot_encoded)\n",
    "    print(inp_train.shape, onehot_encoded_train.shape, inp_val.shape, onehot_encoded_val.shape)\n",
    "    return inp_train, onehot_encoded_train, inp_val, onehot_encoded_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nn_example():\n",
    "    mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "\n",
    "    # Python optimisation variables\n",
    "    learning_rate = 0.5\n",
    "    epochs = 10\n",
    "    batch_size = 100\n",
    "    input_feature_count = 784\n",
    "    out_classes = 10\n",
    "    \n",
    "    # Neural network hidden layer variables\n",
    "    h1 = 500\n",
    "    h2 = 200\n",
    "\n",
    "    # declare the training data placeholders\n",
    "    # input x - for 28 x 28 pixels = 784\n",
    "    x = tf.placeholder(tf.float32, [None, input_feature_count])\n",
    "    # now declare the output data placeholder - 10 digits\n",
    "    y = tf.placeholder(tf.float32, [None, out_classes])\n",
    "\n",
    "    # build the network\n",
    "    keep_prob_input = tf.placeholder(tf.float32)\n",
    "    x_drop = tf.nn.dropout(x, keep_prob=keep_prob_input)\n",
    "\n",
    "    W_fc1 = weight_variable([input_feature_count, h1], tf.float32)\n",
    "    b_fc1 = bias_variable([h1], tf.float32)\n",
    "    h_fc1 = tf.nn.relu(tf.matmul(x_drop, W_fc1) + b_fc1)\n",
    "\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "    h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "    W_fc2 = weight_variable([h1, h2],tf.float32)\n",
    "    b_fc2 = bias_variable([h2], tf.float32)\n",
    "    h_fc2 = tf.nn.relu(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)\n",
    "\n",
    "    h_fc2_drop = tf.nn.dropout(h_fc2, keep_prob)\n",
    "\n",
    "    W_fc3 = weight_variable([h2, out_classes],tf.float32)\n",
    "    b_fc3 = bias_variable([out_classes], tf.float32)\n",
    "\n",
    "    # now calculate the hidden layer output - in this case, let's use a softmax activated\n",
    "    # output layer\n",
    "    y_ = tf.nn.softmax(tf.matmul(h_fc2_drop, W_fc3) + b_fc3)\n",
    "\n",
    "    # now let's define the cost function which we are going to train the model on\n",
    "    y_clipped = tf.clip_by_value(y_, 1e-10, 0.9999999)\n",
    "    cross_entropy = -tf.reduce_mean(tf.reduce_sum(y * tf.log(y_clipped)\n",
    "                                                  + (1 - y) * tf.log(1 - y_clipped), axis=1))\n",
    "\n",
    "    # add an optimiser\n",
    "    optimiser = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cross_entropy)\n",
    "\n",
    "    # finally setup the initialisation operator\n",
    "    init_op = tf.global_variables_initializer()\n",
    "\n",
    "    # define an accuracy assessment operation\n",
    "    correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "    # add a summary to store the accuracy\n",
    "    accuracy_sum = tf.summary.scalar('accuracy', accuracy)\n",
    "\n",
    "    merged = tf.summary.merge([accuracy_sum])\n",
    "    writer = tf.summary.FileWriter(r'C:\\Users\\anant\\dev\\repos\\HPA')\n",
    "    # start the session\n",
    "    with tf.Session() as sess:\n",
    "        # initialise the variables\n",
    "        sess.run(init_op)\n",
    "        total_batch = int(len(mnist.train.labels) / batch_size)\n",
    "        for epoch in range(epochs):\n",
    "            avg_cost = 0\n",
    "            train_acc = 0\n",
    "            for i in range(total_batch):\n",
    "                batch_x, batch_y = mnist.train.next_batch(batch_size=batch_size)\n",
    "                train_acc,_, c = sess.run([accuracy, optimiser, cross_entropy], feed_dict={x: batch_x, y: batch_y, keep_prob_input: 1.0, keep_prob: 1.0})\n",
    "                avg_cost += c / total_batch\n",
    "            print(\"Epoch:\", (epoch + 1), \"Train accuracy =\", \"{:.3f}\".format(train_acc), \"cost =\", \"{:.3f}\".format(avg_cost))\n",
    "            summary = sess.run(merged, feed_dict={x: mnist.test.images, y: mnist.test.labels, keep_prob_input: 1.0, keep_prob: 1.0})\n",
    "            writer.add_summary(summary, epoch)\n",
    "\n",
    "        print(\"\\nTraining complete!\")\n",
    "        writer.add_graph(sess.graph)\n",
    "        print(sess.run(accuracy, feed_dict={x: mnist.test.images, y: mnist.test.labels, keep_prob_input: 1.0, keep_prob: 1.0}))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    nn_example()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ans = ifnp.sin(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [tf_gpu]",
   "language": "python",
   "name": "Python [tf_gpu]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
