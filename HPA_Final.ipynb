{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "Training for b=500 Duration=209\n",
      "Training for b=1000 Duration=164\n",
      "Training for b=2000 Duration=165\n",
      "Training for b=5000 Duration=160\n",
      "Training for b=10000 Duration=148\n",
      "Training for b=20000 Duration=126\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import time\n",
    "import os\n",
    "from pathlib import Path\n",
    "import datetime\n",
    "import csv\n",
    "\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "epoch = datetime.datetime.utcfromtimestamp(0)\n",
    "\n",
    "def unix_time_secs(dt):\n",
    "    return round((dt - epoch).total_seconds())\n",
    "\n",
    "def weight_variable(shape,astype):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.01, dtype = astype)\n",
    "    return tf.Variable(initial, dtype = astype)\n",
    "\n",
    "def bias_variable(shape,astype):\n",
    "    initial = tf.constant(0.0, shape=shape, dtype = astype)\n",
    "    return tf.Variable(initial, dtype = astype)\n",
    "\n",
    "def nn_example(e, b, data_type):\n",
    "    learning_rate = 0.5\n",
    "    epochs = e\n",
    "    batch_size = b\n",
    "    input_feature_count = 784\n",
    "    out_classes = 10\n",
    "    data_type = data_type\n",
    "    \n",
    "    # Neural network hidden layer variables\n",
    "    h1 = 500\n",
    "    h2 = 200\n",
    "\n",
    "    # declare the training data placeholders\n",
    "    # input x - for 28 x 28 pixels = 784\n",
    "    x = tf.placeholder(data_type, [None, input_feature_count])\n",
    "    # now declare the output data placeholder - 10 digits\n",
    "    y = tf.placeholder(data_type, [None, out_classes])\n",
    "\n",
    "    # build the network\n",
    "    keep_prob_input = tf.placeholder(data_type)\n",
    "    x_drop = tf.nn.dropout(x, keep_prob=keep_prob_input)\n",
    "\n",
    "    W_fc1 = weight_variable([input_feature_count, h1], data_type)\n",
    "    b_fc1 = bias_variable([h1], data_type)\n",
    "    h_fc1 = tf.nn.relu(tf.matmul(x_drop, W_fc1) + b_fc1)\n",
    "\n",
    "    keep_prob = tf.placeholder(data_type)\n",
    "    h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "    W_fc2 = weight_variable([h1, h2],data_type)\n",
    "    b_fc2 = bias_variable([h2], data_type)\n",
    "    h_fc2 = tf.nn.relu(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)\n",
    "\n",
    "    h_fc2_drop = tf.nn.dropout(h_fc2, keep_prob)\n",
    "\n",
    "    W_fc3 = weight_variable([h2, out_classes],data_type)\n",
    "    b_fc3 = bias_variable([out_classes], data_type)\n",
    "\n",
    "    # now calculate the hidden layer output - in this case, let's use a softmax activated\n",
    "    # output layer\n",
    "    y_ = tf.nn.softmax(tf.matmul(h_fc2_drop, W_fc3) + b_fc3)\n",
    "\n",
    "    # now let's define the cost function which we are going to train the model on\n",
    "    y_clipped = tf.clip_by_value(y_, 1e-10, 0.9999999)\n",
    "    cross_entropy = -tf.reduce_mean(tf.reduce_sum(y * tf.log(y_clipped)\n",
    "                                                  + (1 - y) * tf.log(1 - y_clipped), axis=1))\n",
    "\n",
    "    # add an optimiser\n",
    "    optimiser = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cross_entropy)\n",
    "\n",
    "    # finally setup the initialisation operator\n",
    "    init_op = tf.global_variables_initializer()\n",
    "\n",
    "    # define an accuracy assessment operation\n",
    "    correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, data_type))\n",
    "\n",
    "    # add a summary to store the accuracy\n",
    "    accuracy_sum = tf.summary.scalar('accuracy', accuracy)\n",
    "    \n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    test_accuracy = 0\n",
    "    # start the session\n",
    "    with tf.Session() as sess:\n",
    "        # initialise the variables\n",
    "        sess.run(init_op)\n",
    "        total_batch = int(len(mnist.train.labels) / batch_size)\n",
    "        start_time = time.time()\n",
    "        for epoch in range(epochs):\n",
    "            avg_cost = 0\n",
    "            train_acc = 0\n",
    "            for i in range(total_batch):\n",
    "                batch_x, batch_y = mnist.train.next_batch(batch_size=batch_size)\n",
    "                train_acc,_, c = sess.run([accuracy, optimiser, cross_entropy], feed_dict={x: batch_x, y: batch_y, keep_prob_input: 1.0, keep_prob: 1.0})\n",
    "                avg_cost += c / total_batch\n",
    "            elapsed = time.time() - start_time\n",
    "            test_accuracy = sess.run(accuracy, feed_dict={x: mnist.test.images, y: mnist.test.labels, keep_prob_input: 1.0, keep_prob: 1.0})\n",
    "#             print((epoch+1), batch_size, \"{:.3f}\".format(avg_cost), \"{:.3f}\".format(test_accuracy), \"{:.3f}\".format(elapsed), sep='\\t')\n",
    "#     return arr_accuracy, time_elapsed, nw_size\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    data_type = tf.float64\n",
    "    filename = r'D:\\UCI\\HPA_Project\\GPU-Z Data\\f64.csv'\n",
    "    file = open(filename, \"w\", newline='\\n')\n",
    "    batch_size = [500, 1000, 2000, 5000, 10000, 20000]\n",
    "    epochs = 15\n",
    "    num_bsize = len(batch_size)\n",
    "    mat = np.zeros((num_bsize, 4))\n",
    "    start_time = 0\n",
    "    for i, b in enumerate(batch_size):\n",
    "        print('Training for b={}'.format(b), end=' ')\n",
    "        start_time = datetime.datetime.now()\n",
    "        nn_example(epochs, b, data_type)\n",
    "        end_time = datetime.datetime.now()\n",
    "        line = str(b)+','+str(unix_time_secs(start_time))+','+str(unix_time_secs(end_time))+','+str(unix_time_secs(end_time)-unix_time_secs(start_time))\n",
    "        file.write(line)\n",
    "        print('Duration={}'.format(unix_time_secs(end_time)-unix_time_secs(start_time)))\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for b=500 Duration=49\n",
      "Training for b=1000 Duration=47\n",
      "Training for b=2000 Duration=43\n",
      "Training for b=5000 Duration=41\n",
      "Training for b=10000 Duration=43\n",
      "Training for b=20000 Duration=38\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    data_type = tf.float32\n",
    "    filename = r'D:\\UCI\\HPA_Project\\GPU-Z Data\\f32.csv'\n",
    "    file = open(filename, \"w\", newline='\\n')\n",
    "    batch_size = [500, 1000, 2000, 5000, 10000, 20000]\n",
    "    epochs = 15\n",
    "    num_bsize = len(batch_size)\n",
    "    mat = np.zeros((num_bsize, 4))\n",
    "    start_time = 0\n",
    "    for i, b in enumerate(batch_size):\n",
    "        print('Training for b={}'.format(b), end=' ')\n",
    "        start_time = datetime.datetime.now()\n",
    "        nn_example(epochs, b, data_type)\n",
    "        end_time = datetime.datetime.now()\n",
    "        line = str(b)+','+str(unix_time_secs(start_time))+','+str(unix_time_secs(end_time))+','+str(unix_time_secs(end_time)-unix_time_secs(start_time))\n",
    "        file.write(line)\n",
    "        print('Duration={}'.format(unix_time_secs(end_time)-unix_time_secs(start_time)))\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for b=500 Duration=45\n",
      "Training for b=1000 Duration=44\n",
      "Training for b=2000 "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    data_type = tf.float16\n",
    "    filename = r'D:\\UCI\\HPA_Project\\GPU-Z Data\\f16.csv'\n",
    "    file = open(filename, \"w\", newline='\\n')\n",
    "    batch_size = [500, 1000, 2000, 5000, 10000, 20000]\n",
    "    epochs = 15\n",
    "    num_bsize = len(batch_size)\n",
    "    mat = np.zeros((num_bsize, 4))\n",
    "    start_time = 0\n",
    "    for i, b in enumerate(batch_size):\n",
    "        print('Training for b={}'.format(b), end=' ')\n",
    "        start_time = datetime.datetime.now()\n",
    "        nn_example(epochs, b, data_type)\n",
    "        end_time = datetime.datetime.now()\n",
    "        line = str(b)+','+str(unix_time_secs(start_time))+','+str(unix_time_secs(end_time))+','+str(unix_time_secs(end_time)-unix_time_secs(start_time))\n",
    "        file.write(line)\n",
    "        print('Duration={}'.format(unix_time_secs(end_time)-unix_time_secs(start_time)))\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017 12 01\n",
      "18 08 00\n",
      "2017-12-01 18:08:00\n",
      "1512151680\n",
      "1512151680\n"
     ]
    }
   ],
   "source": [
    "date='20171201'\n",
    "time='18:08:00'\n",
    "yyyy, MM, dd = date[:4], date[4:6], date[6:]\n",
    "hh, mm, ss = time.split(':')\n",
    "print(yyyy, MM, dd)\n",
    "print(hh, mm, ss)\n",
    "print(datetime.datetime(int(yyyy), int(MM), int(dd), int(hh), int(mm), int(ss)))\n",
    "print(unix_time_secs(datetime.datetime(int(yyyy), int(MM), int(dd), int(hh), int(mm), int(ss))))\n",
    "print(unix_time_secs(datetime.datetime.now()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "Training for b=50\n",
      "Training for b=100\n",
      "Training for b=200\n",
      "Training for b=500\n",
      "Training for b=1000\n",
      "Training for b=2000\n",
      "Training for b=5000\n",
      "Training for b=10000\n",
      "Training for b=20000\n",
      "Training for b=30000\n",
      "Training for b=40000\n",
      "Training for b=50000\n",
      "Training for b=55000\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import time\n",
    "import os\n",
    "from pathlib import Path\n",
    "import datetime\n",
    "\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "\n",
    "def weight_variable(shape,astype):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.01, dtype = astype)\n",
    "    return tf.Variable(initial, dtype = astype)\n",
    "\n",
    "def bias_variable(shape,astype):\n",
    "    initial = tf.constant(0.0, shape=shape, dtype = astype)\n",
    "    return tf.Variable(initial, dtype = astype)\n",
    "\n",
    "def nn_example(e, b, data_type):\n",
    "    learning_rate = 0.5\n",
    "    epochs = e\n",
    "    batch_size = b\n",
    "    input_feature_count = 784\n",
    "    out_classes = 10\n",
    "    data_type = data_type\n",
    "    \n",
    "    # Neural network hidden layer variables\n",
    "    h1 = 500\n",
    "    h2 = 200\n",
    "\n",
    "    # declare the training data placeholders\n",
    "    # input x - for 28 x 28 pixels = 784\n",
    "    x = tf.placeholder(data_type, [None, input_feature_count])\n",
    "    # now declare the output data placeholder - 10 digits\n",
    "    y = tf.placeholder(data_type, [None, out_classes])\n",
    "\n",
    "    # build the network\n",
    "    keep_prob_input = tf.placeholder(data_type)\n",
    "    x_drop = tf.nn.dropout(x, keep_prob=keep_prob_input)\n",
    "\n",
    "    W_fc1 = weight_variable([input_feature_count, h1], data_type)\n",
    "    b_fc1 = bias_variable([h1], data_type)\n",
    "    h_fc1 = tf.nn.relu(tf.matmul(x_drop, W_fc1) + b_fc1)\n",
    "\n",
    "    keep_prob = tf.placeholder(data_type)\n",
    "    h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "    W_fc2 = weight_variable([h1, h2],data_type)\n",
    "    b_fc2 = bias_variable([h2], data_type)\n",
    "    h_fc2 = tf.nn.relu(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)\n",
    "\n",
    "    h_fc2_drop = tf.nn.dropout(h_fc2, keep_prob)\n",
    "\n",
    "    W_fc3 = weight_variable([h2, out_classes],data_type)\n",
    "    b_fc3 = bias_variable([out_classes], data_type)\n",
    "\n",
    "    # now calculate the hidden layer output - in this case, let's use a softmax activated\n",
    "    # output layer\n",
    "    y_ = tf.nn.softmax(tf.matmul(h_fc2_drop, W_fc3) + b_fc3)\n",
    "\n",
    "    # now let's define the cost function which we are going to train the model on\n",
    "    y_clipped = tf.clip_by_value(y_, 1e-10, 0.9999999)\n",
    "    cross_entropy = -tf.reduce_mean(tf.reduce_sum(y * tf.log(y_clipped)\n",
    "                                                  + (1 - y) * tf.log(1 - y_clipped), axis=1))\n",
    "\n",
    "    # add an optimiser\n",
    "    optimiser = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cross_entropy)\n",
    "\n",
    "    # finally setup the initialisation operator\n",
    "    init_op = tf.global_variables_initializer()\n",
    "\n",
    "    # define an accuracy assessment operation\n",
    "    correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, data_type))\n",
    "\n",
    "    # add a summary to store the accuracy\n",
    "    accuracy_sum = tf.summary.scalar('accuracy', accuracy)\n",
    "    \n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    test_accuracy = 0\n",
    "    # start the session\n",
    "    with tf.Session() as sess:\n",
    "        # initialise the variables\n",
    "        sess.run(init_op)\n",
    "        total_batch = int(len(mnist.train.labels) / batch_size)\n",
    "        start_time = time.time()\n",
    "        for epoch in range(epochs):\n",
    "            avg_cost = 0\n",
    "            train_acc = 0\n",
    "            for i in range(total_batch):\n",
    "                batch_x, batch_y = mnist.train.next_batch(batch_size=batch_size)\n",
    "                train_acc,_, c = sess.run([accuracy, optimiser, cross_entropy], feed_dict={x: batch_x, y: batch_y, keep_prob_input: 1.0, keep_prob: 1.0})\n",
    "                avg_cost += c / total_batch\n",
    "            elapsed = time.time() - start_time\n",
    "            test_accuracy = sess.run(accuracy, feed_dict={x: mnist.test.images, y: mnist.test.labels, keep_prob_input: 1.0, keep_prob: 1.0})\n",
    "#             print((epoch+1), batch_size, \"{:.3f}\".format(avg_cost), \"{:.3f}\".format(test_accuracy), \"{:.3f}\".format(elapsed), sep='\\t')\n",
    "#     return arr_accuracy, time_elapsed, nw_size\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    data_type = tf.float32\n",
    "    filename = r'D:\\UCI\\HPA_Project\\GPU-Z Data\\f32.csv'\n",
    "    file = open(filename, \"w\", newline='\\n')\n",
    "    batch_size = [50, 100, 200, 500, 1000, 2000, 5000, 10000, 20000, 30000, 40000, 50000, 55000]\n",
    "    epochs = 15\n",
    "    num_bsize = len(batch_size)\n",
    "    mat = np.zeros((num_bsize, 4))\n",
    "    start_time = 0\n",
    "    for i, b in enumerate(batch_size):\n",
    "        print('Training for b={}'.format(b))\n",
    "        start_time = datetime.datetime.now()\n",
    "        nn_example(epochs, b, data_type)\n",
    "        end_time = datetime.datetime.now()\n",
    "        line = str(b)+','+str(unix_time_secs(start_time))+','+str(unix_time_secs(end_time))+','+str(unix_time_secs(end_time)-unix_time_secs(start_time))\n",
    "        file.write(line)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "Training for b=50\n",
      "1\t50\tnan\t0.098\t11.641\n",
      "2\t50\tnan\t0.098\t23.466\n",
      "3\t50\tnan\t0.098\t34.487\n",
      "4\t50\tnan\t0.098\t45.866\n",
      "5\t50\tnan\t0.098\t56.040\n",
      "6\t50\tnan\t0.098\t66.232\n",
      "7\t50\tnan\t0.098\t80.117\n",
      "8\t50\tnan\t0.098\t90.618\n",
      "9\t50\tnan\t0.098\t101.140\n",
      "10\t50\tnan\t0.098\t111.472\n",
      "11\t50\tnan\t0.098\t121.899\n",
      "12\t50\tnan\t0.098\t132.334\n",
      "13\t50\tnan\t0.098\t143.207\n",
      "14\t50\tnan\t0.098\t160.933\n",
      "15\t50\tnan\t0.098\t174.820\n",
      "Training for b=100\n",
      "1\t100\tnan\t0.098\t4.480\n",
      "2\t100\tnan\t0.098\t8.662\n",
      "3\t100\tnan\t0.098\t12.637\n",
      "4\t100\tnan\t0.098\t16.534\n",
      "5\t100\tnan\t0.098\t20.525\n",
      "6\t100\tnan\t0.098\t24.496\n",
      "7\t100\tnan\t0.098\t28.442\n",
      "8\t100\tnan\t0.098\t32.403\n",
      "9\t100\tnan\t0.098\t36.335\n",
      "10\t100\tnan\t0.098\t40.317\n",
      "11\t100\tnan\t0.098\t44.772\n",
      "12\t100\tnan\t0.098\t50.616\n",
      "13\t100\tnan\t0.098\t55.687\n",
      "14\t100\tnan\t0.098\t59.765\n",
      "15\t100\tnan\t0.098\t63.736\n",
      "Training for b=200\n",
      "1\t200\tnan\t0.098\t2.436\n",
      "2\t200\tnan\t0.098\t4.969\n",
      "3\t200\tnan\t0.098\t7.424\n",
      "4\t200\tnan\t0.098\t9.896\n",
      "5\t200\tnan\t0.098\t12.747\n",
      "6\t200\tnan\t0.098\t16.248\n",
      "7\t200\tnan\t0.098\t19.686\n",
      "8\t200\tnan\t0.098\t23.121\n",
      "9\t200\tnan\t0.098\t25.896\n",
      "10\t200\tnan\t0.098\t28.295\n",
      "11\t200\tnan\t0.098\t30.715\n",
      "12\t200\tnan\t0.098\t33.151\n",
      "13\t200\tnan\t0.098\t35.790\n",
      "14\t200\tnan\t0.098\t39.309\n",
      "15\t200\tnan\t0.098\t42.776\n",
      "Training for b=500\n",
      "1\t500\tnan\t0.098\t2.669\n",
      "2\t500\tnan\t0.098\t5.025\n",
      "3\t500\tnan\t0.098\t7.229\n",
      "4\t500\tnan\t0.098\t9.397\n",
      "5\t500\tnan\t0.098\t11.619\n",
      "6\t500\tnan\t0.098\t13.742\n",
      "7\t500\tnan\t0.098\t15.931\n",
      "8\t500\tnan\t0.098\t18.070\n",
      "9\t500\tnan\t0.098\t20.312\n",
      "10\t500\tnan\t0.098\t22.486\n",
      "11\t500\tnan\t0.098\t24.630\n",
      "12\t500\tnan\t0.098\t26.770\n",
      "13\t500\tnan\t0.098\t28.918\n",
      "14\t500\tnan\t0.098\t31.063\n",
      "15\t500\tnan\t0.098\t33.245\n",
      "Training for b=1000\n",
      "1\t1000\tnan\t0.098\t1.863\n",
      "2\t1000\tnan\t0.098\t3.848\n",
      "3\t1000\tnan\t0.098\t5.646\n",
      "4\t1000\tnan\t0.098\t7.457\n",
      "5\t1000\tnan\t0.098\t9.217\n",
      "6\t1000\tnan\t0.098\t11.003\n",
      "7\t1000\tnan\t0.098\t12.762\n",
      "8\t1000\tnan\t0.098\t14.540\n",
      "9\t1000\tnan\t0.098\t16.353\n",
      "10\t1000\tnan\t0.098\t18.136\n",
      "11\t1000\tnan\t0.098\t19.897\n",
      "12\t1000\tnan\t0.098\t21.688\n",
      "13\t1000\tnan\t0.098\t23.474\n",
      "14\t1000\tnan\t0.098\t25.286\n",
      "15\t1000\tnan\t0.098\t27.068\n",
      "Training for b=2000\n",
      "1\t2000\t3.243\t0.288\t1.618\n",
      "2\t2000\tnan\t0.098\t3.330\n",
      "3\t2000\tnan\t0.098\t4.847\n",
      "4\t2000\tnan\t0.098\t6.338\n",
      "5\t2000\tnan\t0.098\t7.911\n",
      "6\t2000\tnan\t0.098\t9.469\n",
      "7\t2000\tnan\t0.098\t10.987\n",
      "8\t2000\tnan\t0.098\t12.525\n",
      "9\t2000\tnan\t0.098\t14.094\n",
      "10\t2000\tnan\t0.098\t15.677\n",
      "11\t2000\tnan\t0.098\t17.232\n",
      "12\t2000\tnan\t0.098\t18.784\n",
      "13\t2000\tnan\t0.098\t20.364\n",
      "14\t2000\tnan\t0.098\t21.947\n",
      "15\t2000\tnan\t0.098\t23.482\n",
      "Training for b=5000\n",
      "1\t5000\t3.249\t0.114\t1.466\n",
      "2\t5000\t3.243\t0.132\t3.083\n",
      "3\t5000\t3.205\t0.265\t4.463\n",
      "4\t5000\tnan\t0.098\t5.868\n",
      "5\t5000\tnan\t0.098\t7.275\n",
      "6\t5000\tnan\t0.098\t8.657\n",
      "7\t5000\tnan\t0.098\t10.065\n",
      "8\t5000\tnan\t0.098\t11.478\n",
      "9\t5000\tnan\t0.098\t12.844\n",
      "10\t5000\tnan\t0.098\t14.253\n",
      "11\t5000\tnan\t0.098\t15.649\n",
      "12\t5000\tnan\t0.098\t17.022\n",
      "13\t5000\tnan\t0.098\t18.416\n",
      "14\t5000\tnan\t0.098\t19.829\n",
      "15\t5000\tnan\t0.098\t21.227\n",
      "Training for b=10000\n",
      "1\t10000\t3.250\t0.114\t1.374\n",
      "2\t10000\t3.248\t0.114\t2.855\n",
      "3\t10000\t3.246\t0.114\t4.147\n",
      "4\t10000\t3.244\t0.114\t5.411\n",
      "5\t10000\t3.238\t0.141\t6.669\n",
      "6\t10000\t3.225\t0.280\t7.928\n",
      "7\t10000\t3.181\t0.272\t9.217\n",
      "8\t10000\t3.014\t0.364\t10.256\n",
      "9\t10000\tnan\t0.098\t11.518\n",
      "10\t10000\tnan\t0.098\t12.805\n",
      "11\t10000\tnan\t0.098\t13.940\n",
      "12\t10000\tnan\t0.098\t15.212\n",
      "13\t10000\tnan\t0.098\t16.463\n",
      "14\t10000\tnan\t0.098\t17.724\n",
      "15\t10000\tnan\t0.098\t19.132\n",
      "Training for b=20000\n",
      "1\t20000\t3.251\t0.114\t1.204\n",
      "2\t20000\t3.250\t0.114\t2.260\n",
      "3\t20000\t3.248\t0.114\t3.354\n",
      "4\t20000\t3.248\t0.114\t4.431\n",
      "5\t20000\t3.248\t0.114\t5.271\n",
      "6\t20000\t3.247\t0.114\t6.355\n",
      "7\t20000\t3.246\t0.114\t7.470\n",
      "8\t20000\t3.245\t0.114\t8.583\n",
      "9\t20000\t3.244\t0.114\t9.426\n",
      "10\t20000\t3.241\t0.114\t10.505\n",
      "11\t20000\t3.239\t0.116\t11.680\n",
      "12\t20000\t3.235\t0.143\t12.768\n",
      "13\t20000\t3.230\t0.218\t13.616\n",
      "14\t20000\t3.222\t0.290\t14.723\n",
      "15\t20000\t3.210\t0.312\t15.802\n",
      "Training for b=30000\n",
      "1\t30000\t3.252\t0.114\t0.809\n",
      "2\t30000\t3.250\t0.114\t1.974\n",
      "3\t30000\t3.250\t0.114\t2.632\n",
      "4\t30000\t3.250\t0.114\t3.565\n",
      "5\t30000\t3.248\t0.114\t4.228\n",
      "6\t30000\t3.248\t0.114\t5.158\n",
      "7\t30000\t3.248\t0.114\t6.098\n",
      "8\t30000\t3.248\t0.114\t6.781\n",
      "9\t30000\t3.248\t0.114\t7.712\n",
      "10\t30000\t3.248\t0.114\t8.375\n",
      "11\t30000\t3.246\t0.114\t9.284\n",
      "12\t30000\t3.246\t0.114\t9.961\n",
      "13\t30000\t3.246\t0.114\t10.896\n",
      "14\t30000\t3.244\t0.114\t11.552\n",
      "15\t30000\t3.246\t0.114\t12.467\n",
      "Training for b=40000\n",
      "1\t40000\t3.252\t0.115\t1.268\n",
      "2\t40000\t3.250\t0.114\t2.374\n",
      "3\t40000\t3.250\t0.114\t3.494\n",
      "4\t40000\t3.250\t0.114\t4.673\n",
      "5\t40000\t3.248\t0.114\t5.818\n",
      "6\t40000\t3.248\t0.114\t6.647\n",
      "7\t40000\t3.248\t0.114\t7.786\n",
      "8\t40000\t3.248\t0.114\t8.939\n",
      "9\t40000\t3.248\t0.114\t9.801\n",
      "10\t40000\t3.248\t0.114\t10.934\n",
      "11\t40000\t3.248\t0.114\t12.089\n",
      "12\t40000\t3.246\t0.114\t13.265\n",
      "13\t40000\t3.246\t0.114\t14.121\n",
      "14\t40000\t3.246\t0.114\t15.265\n",
      "15\t40000\t3.246\t0.114\t16.416\n",
      "Training for b=50000\n",
      "1\t50000\t3.252\t0.114\t1.471\n",
      "2\t50000\t3.250\t0.114\t3.129\n",
      "3\t50000\t3.250\t0.114\t4.497\n",
      "4\t50000\t3.250\t0.114\t5.572\n",
      "5\t50000\t3.248\t0.114\t6.907\n",
      "6\t50000\t3.248\t0.114\t8.286\n",
      "7\t50000\t3.248\t0.114\t9.805\n",
      "8\t50000\t3.248\t0.114\t11.385\n",
      "9\t50000\t3.248\t0.114\t12.837\n",
      "10\t50000\t3.248\t0.114\t14.208\n",
      "11\t50000\t3.248\t0.114\t15.574\n",
      "12\t50000\t3.248\t0.114\t16.919\n",
      "13\t50000\t3.246\t0.114\t18.284\n",
      "14\t50000\t3.246\t0.114\t19.646\n",
      "15\t50000\t3.246\t0.114\t20.718\n",
      "Training for b=55000\n",
      "1\t55000\t3.252\t0.114\t1.596\n",
      "2\t55000\t3.250\t0.114\t3.331\n",
      "3\t55000\t3.250\t0.114\t4.878\n",
      "4\t55000\t3.250\t0.114\t6.354\n",
      "5\t55000\t3.248\t0.114\t7.686\n",
      "6\t55000\t3.248\t0.114\t8.894\n",
      "7\t55000\t3.248\t0.114\t10.100\n",
      "8\t55000\t3.248\t0.114\t11.319\n",
      "9\t55000\t3.248\t0.114\t12.526\n",
      "10\t55000\t3.248\t0.114\t13.729\n",
      "11\t55000\t3.248\t0.114\t14.933\n",
      "12\t55000\t3.248\t0.114\t16.141\n",
      "13\t55000\t3.248\t0.114\t17.353\n",
      "14\t55000\t3.246\t0.114\t18.562\n",
      "15\t55000\t3.246\t0.114\t19.767\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import time\n",
    "import os\n",
    "from pathlib import Path\n",
    "import datetime\n",
    "\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "\n",
    "def weight_variable(shape,astype):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.01, dtype = astype)\n",
    "    return tf.Variable(initial, dtype = astype)\n",
    "\n",
    "def bias_variable(shape,astype):\n",
    "    initial = tf.constant(0.0, shape=shape, dtype = astype)\n",
    "    return tf.Variable(initial, dtype = astype)\n",
    "\n",
    "def nn_example(e, b, data_type):\n",
    "    learning_rate = 0.5\n",
    "    epochs = e\n",
    "    batch_size = b\n",
    "    input_feature_count = 784\n",
    "    out_classes = 10\n",
    "    data_type = data_type\n",
    "    \n",
    "    # Neural network hidden layer variables\n",
    "    h1 = 500\n",
    "    h2 = 200\n",
    "\n",
    "    # declare the training data placeholders\n",
    "    # input x - for 28 x 28 pixels = 784\n",
    "    x = tf.placeholder(data_type, [None, input_feature_count])\n",
    "    # now declare the output data placeholder - 10 digits\n",
    "    y = tf.placeholder(data_type, [None, out_classes])\n",
    "\n",
    "    # build the network\n",
    "    keep_prob_input = tf.placeholder(data_type)\n",
    "    x_drop = tf.nn.dropout(x, keep_prob=keep_prob_input)\n",
    "\n",
    "    W_fc1 = weight_variable([input_feature_count, h1], data_type)\n",
    "    b_fc1 = bias_variable([h1], data_type)\n",
    "    h_fc1 = tf.nn.relu(tf.matmul(x_drop, W_fc1) + b_fc1)\n",
    "\n",
    "    keep_prob = tf.placeholder(data_type)\n",
    "    h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "    W_fc2 = weight_variable([h1, h2],data_type)\n",
    "    b_fc2 = bias_variable([h2], data_type)\n",
    "    h_fc2 = tf.nn.relu(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)\n",
    "\n",
    "    h_fc2_drop = tf.nn.dropout(h_fc2, keep_prob)\n",
    "\n",
    "    W_fc3 = weight_variable([h2, out_classes],data_type)\n",
    "    b_fc3 = bias_variable([out_classes], data_type)\n",
    "\n",
    "    # now calculate the hidden layer output - in this case, let's use a softmax activated\n",
    "    # output layer\n",
    "    y_ = tf.nn.softmax(tf.matmul(h_fc2_drop, W_fc3) + b_fc3)\n",
    "\n",
    "    # now let's define the cost function which we are going to train the model on\n",
    "    y_clipped = tf.clip_by_value(y_, 1e-10, 0.9999999)\n",
    "    cross_entropy = -tf.reduce_mean(tf.reduce_sum(y * tf.log(y_clipped)\n",
    "                                                  + (1 - y) * tf.log(1 - y_clipped), axis=1))\n",
    "\n",
    "    # add an optimiser\n",
    "    optimiser = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cross_entropy)\n",
    "\n",
    "    # finally setup the initialisation operator\n",
    "    init_op = tf.global_variables_initializer()\n",
    "\n",
    "    # define an accuracy assessment operation\n",
    "    correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, data_type))\n",
    "\n",
    "    # add a summary to store the accuracy\n",
    "    accuracy_sum = tf.summary.scalar('accuracy', accuracy)\n",
    "    \n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    test_accuracy = 0\n",
    "    # start the session\n",
    "    with tf.Session() as sess:\n",
    "        # initialise the variables\n",
    "        sess.run(init_op)\n",
    "        total_batch = int(len(mnist.train.labels) / batch_size)\n",
    "        start_time = time.time()\n",
    "        for epoch in range(epochs):\n",
    "            avg_cost = 0\n",
    "            train_acc = 0\n",
    "            for i in range(total_batch):\n",
    "                batch_x, batch_y = mnist.train.next_batch(batch_size=batch_size)\n",
    "                train_acc,_, c = sess.run([accuracy, optimiser, cross_entropy], feed_dict={x: batch_x, y: batch_y, keep_prob_input: 1.0, keep_prob: 1.0})\n",
    "                avg_cost += c / total_batch\n",
    "            elapsed = time.time() - start_time\n",
    "            test_accuracy = sess.run(accuracy, feed_dict={x: mnist.test.images, y: mnist.test.labels, keep_prob_input: 1.0, keep_prob: 1.0})\n",
    "            print((epoch+1), batch_size, \"{:.3f}\".format(avg_cost), \"{:.3f}\".format(test_accuracy), \"{:.3f}\".format(elapsed), sep='\\t')\n",
    "#     return arr_accuracy, time_elapsed, nw_size\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    data_type = tf.float16\n",
    "    filename = r'D:\\UCI\\HPA_Project\\GPU-Z Data\\f16.csv'\n",
    "    file = open(filename, \"w\", newline='\\n')\n",
    "    batch_size = [50, 100, 200, 500, 1000, 2000, 5000, 10000, 20000, 30000]\n",
    "    epochs = 15\n",
    "    num_bsize = len(batch_size)\n",
    "    mat = np.zeros((num_bsize, 4))\n",
    "    start_time = 0\n",
    "    for i, b in enumerate(batch_size):\n",
    "        print('Training for b={}'.format(b))\n",
    "        start_time = datetime.datetime.now()\n",
    "        nn_example(epochs, b, data_type)\n",
    "        end_time = datetime.datetime.now()\n",
    "        line = str(b)+','+str(unix_time_secs(start_time))+','+str(unix_time_secs(end_time))+','+str(unix_time_secs(end_time)-unix_time_secs(start_time))+'\\n'\n",
    "        file.write(line)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import time\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "\n",
    "def weight_variable(shape,astype):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.01, dtype = astype)\n",
    "    return tf.Variable(initial, dtype = astype)\n",
    "\n",
    "def bias_variable(shape,astype):\n",
    "    initial = tf.constant(0.0, shape=shape, dtype = astype)\n",
    "    return tf.Variable(initial, dtype = astype)\n",
    "\n",
    "def nn_example(e, b, data_type):\n",
    "    learning_rate = 0.5\n",
    "    epochs = e\n",
    "    batch_size = b\n",
    "    input_feature_count = 784\n",
    "    out_classes = 10\n",
    "    data_type = data_type\n",
    "    max_epoch = epochs[-1]\n",
    "    \n",
    "    # Neural network hidden layer variables\n",
    "    h1 = 50\n",
    "    h2 = 20\n",
    "\n",
    "    # declare the training data placeholders\n",
    "    # input x - for 28 x 28 pixels = 784\n",
    "    x = tf.placeholder(data_type, [None, input_feature_count])\n",
    "    # now declare the output data placeholder - 10 digits\n",
    "    y = tf.placeholder(data_type, [None, out_classes])\n",
    "\n",
    "    # build the network\n",
    "    keep_prob_input = tf.placeholder(data_type)\n",
    "    x_drop = tf.nn.dropout(x, keep_prob=keep_prob_input)\n",
    "\n",
    "    W_fc1 = weight_variable([input_feature_count, h1], data_type)\n",
    "    b_fc1 = bias_variable([h1], data_type)\n",
    "    h_fc1 = tf.nn.relu(tf.matmul(x_drop, W_fc1) + b_fc1)\n",
    "\n",
    "    keep_prob = tf.placeholder(data_type)\n",
    "    h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "    W_fc2 = weight_variable([h1, h2],data_type)\n",
    "    b_fc2 = bias_variable([h2], data_type)\n",
    "    h_fc2 = tf.nn.relu(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)\n",
    "\n",
    "    h_fc2_drop = tf.nn.dropout(h_fc2, keep_prob)\n",
    "\n",
    "    W_fc3 = weight_variable([h2, out_classes],data_type)\n",
    "    b_fc3 = bias_variable([out_classes], data_type)\n",
    "\n",
    "    # now calculate the hidden layer output - in this case, let's use a softmax activated\n",
    "    # output layer\n",
    "    y_ = tf.nn.softmax(tf.matmul(h_fc2_drop, W_fc3) + b_fc3)\n",
    "\n",
    "    # now let's define the cost function which we are going to train the model on\n",
    "    y_clipped = tf.clip_by_value(y_, 1e-10, 0.9999999)\n",
    "    cross_entropy = -tf.reduce_mean(tf.reduce_sum(y * tf.log(y_clipped)\n",
    "                                                  + (1 - y) * tf.log(1 - y_clipped), axis=1))\n",
    "\n",
    "    # add an optimiser\n",
    "    optimiser = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cross_entropy)\n",
    "\n",
    "    # finally setup the initialisation operator\n",
    "    init_op = tf.global_variables_initializer()\n",
    "\n",
    "    # define an accuracy assessment operation\n",
    "    correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, data_type))\n",
    "\n",
    "    # add a summary to store the accuracy\n",
    "    accuracy_sum = tf.summary.scalar('accuracy', accuracy)\n",
    "    \n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    merged = tf.summary.merge([accuracy_sum])\n",
    "    writer = tf.summary.FileWriter(r'C:\\Users\\anant\\dev\\repos\\HPA')\n",
    "    \n",
    "    test_accuracy = 0\n",
    "    # start the session\n",
    "    with tf.Session() as sess:\n",
    "        # initialise the variables\n",
    "        sess.run(init_op)\n",
    "        total_batch = int(len(mnist.train.labels) / batch_size)\n",
    "        start_time = time.time()\n",
    "        for epoch in range(max_epoch+1):\n",
    "            avg_cost = 0\n",
    "            train_acc = 0\n",
    "            for i in range(total_batch):\n",
    "                batch_x, batch_y = mnist.train.next_batch(batch_size=batch_size)\n",
    "                train_acc,_, c = sess.run([accuracy, optimiser, cross_entropy], feed_dict={x: batch_x, y: batch_y, keep_prob_input: 1.0, keep_prob: 1.0})\n",
    "                avg_cost += c / total_batch\n",
    "            if(epoch in epochs):\n",
    "                elapsed = time.time() - start_time\n",
    "#                 filepath = \"D:\\\\UCI\\\\HPA_Project\\\\weights_data\\\\epoch_\"+ str(epoch)+ '_batch_' +str(batch_size)+'_'+str(data_type)\n",
    "#                 save_path = saver.save(sess, filepath)\n",
    "#                 mfilepath = filepath + \".data-00000-of-00001\"\n",
    "#                 file = Path(mfilepath)\n",
    "#                 size = file.stat().st_size\n",
    "                test_accuracy = sess.run(accuracy, feed_dict={x: mnist.test.images, y: mnist.test.labels, keep_prob_input: 1.0, keep_prob: 1.0})\n",
    "                print(epoch, batch_size, test_accuracy, elapsed)\n",
    "#     return arr_accuracy, time_elapsed, nw_size\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    data_type = tf.float32\n",
    "    batch_size = [50, 100, 200, 210, 220, 230, 240, 250, 260, 270, 280, 290, 300, 400, 500]\n",
    "#     epochs = [5,10, 15, 20] #, 30, 50] #, 80, 100, 150, 200, 300, 500, 700, 1000]\n",
    "    epochs = [15]\n",
    "    num_epochs = len(epochs)\n",
    "    num_bsize = len(batch_size)\n",
    "    for j, b in enumerate(batch_size):\n",
    "        nn_example(epochs, b, data_type)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import matplotlib as plt\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "accuracy_mat_64 = accuracy_mat\n",
    "elapsed_time_64 = elapsed_time\n",
    "size_mat_64 = size_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "accuracy_mat_32 = accuracy_mat\n",
    "elapsed_time_32 = elapsed_time\n",
    "size_mat_32 = size_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(accuracy_mat)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [tf_gpu]",
   "language": "python",
   "name": "Python [tf_gpu]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
